travis_fold:start:worker_info
[0K[33;1mWorker information[0m
hostname: f6d50ca0-fbec-4ccb-a324-4f329201fc3a@1.production-1-worker-org-gce-ddb7
version: v6.2.0 https://github.com/travis-ci/worker/tree/5e5476e01646095f48eec13196fdb3faf8f5cbf7
instance: travis-job-94c84eda-d6a3-408d-97cc-f0c7c6a94056 travis-ci-garnet-trusty-1512502259-986baf0 (via amqp)
startup: 6.346707984s
travis_fold:end:worker_info
[0Ktravis_fold:start:system_info
[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: trusty
Build id: 508141431
Job id: 508141432
Runtime kernel version: 4.4.0-101-generic
travis-build version: d6b12fc73
[34m[1mBuild image provisioning date and time[0m
Tue Dec  5 19:58:13 UTC 2017
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
[34m[1mCookbooks Version[0m
7c2c6a6 https://github.com/travis-ci/travis-cookbooks/tree/7c2c6a6
[34m[1mgit version[0m
git version 2.15.1
[34m[1mbash version[0m
GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)
[34m[1mgcc version[0m
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mdocker version[0m
Client:
 Version:      17.09.0-ce
 API version:  1.32
 Go version:   go1.8.3
 Git commit:   afdb6d4
 Built:        Tue Sep 26 22:42:38 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.09.0-ce
 API version:  1.32 (minimum version 1.12)
 Go version:   go1.8.3
 Git commit:   afdb6d4
 Built:        Tue Sep 26 22:41:20 2017
 OS/Arch:      linux/amd64
 Experimental: false
[34m[1mclang version[0m
clang version 5.0.0 (tags/RELEASE_500/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/local/clang-5.0.0/bin
[34m[1mjq version[0m
jq-1.5
[34m[1mbats version[0m
Bats 0.4.0
[34m[1mshellcheck version[0m
0.4.6
[34m[1mshfmt version[0m
v2.0.0
[34m[1mccache version[0m
ccache version 3.1.9

Copyright (C) 2002-2007 Andrew Tridgell
Copyright (C) 2009-2011 Joel Rosdahl

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.
[34m[1mcmake version[0m
cmake version 3.9.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).
[34m[1mheroku version[0m
heroku-cli/6.14.39-addc925 (linux-x64) node-v9.2.0
[34m[1mimagemagick version[0m
Version: ImageMagick 6.7.7-10 2017-07-31 Q16 http://www.imagemagick.org
[34m[1mmd5deep version[0m
4.2
[34m[1mmercurial version[0m
Mercurial Distributed SCM (version 4.2.2)
(see https://mercurial-scm.org for more information)

Copyright (C) 2005-2017 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
[34m[1mmysql version[0m
mysql  Ver 14.14 Distrib 5.6.33, for debian-linux-gnu (x86_64) using  EditLine wrapper
[34m[1mopenssl version[0m
OpenSSL 1.0.1f 6 Jan 2014
[34m[1mpacker version[0m
Packer v1.0.2

Your version of Packer is out of date! The latest version
is 1.1.2. You can update by downloading from www.packer.io
[34m[1mpostgresql client version[0m
psql (PostgreSQL) 9.6.6
[34m[1mragel version[0m
Ragel State Machine Compiler version 6.8 Feb 2013
Copyright (c) 2001-2009 by Adrian Thurston
[34m[1msubversion version[0m
svn, version 1.8.8 (r1568071)
   compiled Aug 10 2017, 17:20:39 on x86_64-pc-linux-gnu

Copyright (C) 2013 The Apache Software Foundation.
This software consists of contributions made by many people;
see the NOTICE file for more information.
Subversion is open source software, see http://subversion.apache.org/

The following repository access (RA) modules are available:

* ra_svn : Module for accessing a repository using the svn network protocol.
  - with Cyrus SASL authentication
  - handles 'svn' scheme
* ra_local : Module for accessing a repository on local disk.
  - handles 'file' scheme
* ra_serf : Module for accessing a repository via WebDAV protocol using serf.
  - using serf 1.3.3
  - handles 'http' scheme
  - handles 'https' scheme

[34m[1msudo version[0m
Sudo version 1.8.9p5
Configure options: --prefix=/usr -v --with-all-insults --with-pam --with-fqdn --with-logging=syslog --with-logfac=authpriv --with-env-editor --with-editor=/usr/bin/editor --with-timeout=15 --with-password-timeout=0 --with-passprompt=[sudo] password for %p:  --without-lecture --with-tty-tickets --disable-root-mailer --enable-admin-flag --with-sendmail=/usr/sbin/sendmail --with-timedir=/var/lib/sudo --mandir=/usr/share/man --libexecdir=/usr/lib/sudo --with-sssd --with-sssd-lib=/usr/lib/x86_64-linux-gnu --with-selinux
Sudoers policy plugin version 1.8.9p5
Sudoers file grammar version 43

Sudoers path: /etc/sudoers
Authentication methods: 'pam'
Syslog facility if syslog is being used for logging: authpriv
Syslog priority to use when user authenticates successfully: notice
Syslog priority to use when user authenticates unsuccessfully: alert
Send mail if the user is not in sudoers
Use a separate timestamp for each user/tty combo
Lecture user the first time they run sudo
Root may run sudo
Allow some information gathering to give useful error messages
Require fully-qualified hostnames in the sudoers file
Visudo will honor the EDITOR environment variable
Set the LOGNAME and USER environment variables
Length at which to wrap log file lines (0 for no wrap): 80
Authentication timestamp timeout: 15.0 minutes
Password prompt timeout: 0.0 minutes
Number of tries to enter a password: 3
Umask to use or 0777 to use user's: 022
Path to mail program: /usr/sbin/sendmail
Flags for mail program: -t
Address to send mail to: root
Subject line for mail messages: *** SECURITY information for %h ***
Incorrect password message: Sorry, try again.
Path to authentication timestamp dir: /var/lib/sudo
Default password prompt: [sudo] password for %p: 
Default user to run commands as: root
Value to override user's $PATH with: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin
Path to the editor for use by visudo: /usr/bin/editor
When to require a password for 'list' pseudocommand: any
When to require a password for 'verify' pseudocommand: all
File descriptors >= 3 will be closed before executing a command
Environment variables to check for sanity:
	TZ
	TERM
	LINGUAS
	LC_*
	LANGUAGE
	LANG
	COLORTERM
Environment variables to remove:
	RUBYOPT
	RUBYLIB
	PYTHONUSERBASE
	PYTHONINSPECT
	PYTHONPATH
	PYTHONHOME
	TMPPREFIX
	ZDOTDIR
	READNULLCMD
	NULLCMD
	FPATH
	PERL5DB
	PERL5OPT
	PERL5LIB
	PERLLIB
	PERLIO_DEBUG 
	JAVA_TOOL_OPTIONS
	SHELLOPTS
	GLOBIGNORE
	PS4
	BASH_ENV
	ENV
	TERMCAP
	TERMPATH
	TERMINFO_DIRS
	TERMINFO
	_RLD*
	LD_*
	PATH_LOCALE
	NLSPATH
	HOSTALIASES
	RES_OPTIONS
	LOCALDOMAIN
	CDPATH
	IFS
Environment variables to preserve:
	JAVA_HOME
	TRAVIS
	CI
	DEBIAN_FRONTEND
	XAUTHORIZATION
	XAUTHORITY
	PS2
	PS1
	PATH
	LS_COLORS
	KRB5CCNAME
	HOSTNAME
	HOME
	DISPLAY
	COLORS
Locale to use while parsing sudoers: C
Directory in which to store input/output logs: /var/log/sudo-io
File in which to store the input/output log: %{seq}
Add an entry to the utmp/utmpx file when allocating a pty
PAM service name to use
PAM service name to use for login shells
Create a new PAM session for the command to run in
Maximum I/O log sequence number: 0

Local IP address and netmask pairs:
	10.240.0.28/255.255.255.255
	172.17.0.1/255.255.0.0

Sudoers I/O plugin version 1.8.9p5
[34m[1mgzip version[0m
gzip 1.6
Copyright (C) 2007, 2010, 2011 Free Software Foundation, Inc.
Copyright (C) 1993 Jean-loup Gailly.
This is free software.  You may redistribute copies of it under the terms of
the GNU General Public License <http://www.gnu.org/licenses/gpl.html>.
There is NO WARRANTY, to the extent permitted by law.

Written by Jean-loup Gailly.
[34m[1mzip version[0m
Copyright (c) 1990-2008 Info-ZIP - Type 'zip "-L"' for software license.
This is Zip 3.0 (July 5th 2008), by Info-ZIP.
Currently maintained by E. Gordon.  Please send bug reports to
the authors using the web page at www.info-zip.org; see README for details.

Latest sources and executables are at ftp://ftp.info-zip.org/pub/infozip,
as of above date; see http://www.info-zip.org/ for other sites.

Compiled with gcc 4.8.2 for Unix (Linux ELF) on Oct 21 2013.

Zip special compilation options:
	USE_EF_UT_TIME       (store Universal Time)
	BZIP2_SUPPORT        (bzip2 library version 1.0.6, 6-Sept-2010)
	    bzip2 code and library copyright (c) Julian R Seward
	    (See the bzip2 license for terms of use)
	SYMLINK_SUPPORT      (symbolic links supported)
	LARGE_FILE_SUPPORT   (can read and write large files on file system)
	ZIP64_SUPPORT        (use Zip64 to store large files in archives)
	UNICODE_SUPPORT      (store and read UTF-8 Unicode paths)
	STORE_UNIX_UIDs_GIDs (store UID/GID sizes/values using new extra field)
	UIDGID_NOT_16BIT     (old Unix 16-bit UID/GID extra field not used)
	[encryption, version 2.91 of 05 Jan 2007] (modified for Zip 3)

Encryption notice:
	The encryption code of this program is not copyrighted and is
	put in the public domain.  It was originally written in Europe
	and, to the best of our knowledge, can be freely distributed
	in both source and object forms from any country, including
	the USA under License Exception TSU of the U.S. Export
	Administration Regulations (section 740.13(e)) of 6 June 2002.

Zip environment options:
             ZIP:  [none]
          ZIPOPT:  [none]
[34m[1mvim version[0m
VIM - Vi IMproved 7.4 (2013 Aug 10, compiled Nov 24 2016 16:43:18)
Included patches: 1-52
Extra patches: 8.0.0056
Modified by pkg-vim-maintainers@lists.alioth.debian.org
Compiled by buildd@
Huge version without GUI.  Features included (+) or not (-):
+acl             +farsi           +mouse_netterm   +syntax
+arabic          +file_in_path    +mouse_sgr       +tag_binary
+autocmd         +find_in_path    -mouse_sysmouse  +tag_old_static
-balloon_eval    +float           +mouse_urxvt     -tag_any_white
-browse          +folding         +mouse_xterm     -tcl
++builtin_terms  -footer          +multi_byte      +terminfo
+byte_offset     +fork()          +multi_lang      +termresponse
+cindent         +gettext         -mzscheme        +textobjects
-clientserver    -hangul_input    +netbeans_intg   +title
-clipboard       +iconv           +path_extra      -toolbar
+cmdline_compl   +insert_expand   -perl            +user_commands
+cmdline_hist    +jumplist        +persistent_undo +vertsplit
+cmdline_info    +keymap          +postscript      +virtualedit
+comments        +langmap         +printer         +visual
+conceal         +libcall         +profile         +visualextra
+cryptv          +linebreak       +python          +viminfo
+cscope          +lispindent      -python3         +vreplace
+cursorbind      +listcmds        +quickfix        +wildignore
+cursorshape     +localmap        +reltime         +wildmenu
+dialog_con      -lua             +rightleft       +windows
+diff            +menu            -ruby            +writebackup
+digraphs        +mksession       +scrollbind      -X11
-dnd             +modify_fname    +signs           -xfontset
-ebcdic          +mouse           +smartindent     -xim
+emacs_tags      -mouseshape      -sniff           -xsmp
+eval            +mouse_dec       +startuptime     -xterm_clipboard
+ex_extra        +mouse_gpm       +statusline      -xterm_save
+extra_search    -mouse_jsbterm   -sun_workshop    -xpm
   system vimrc file: "$VIM/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H     -g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1      
Linking: gcc   -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,--as-needed -o vim        -lm -ltinfo -lnsl  -lselinux  -lacl -lattr -lgpm -ldl    -L/usr/lib/python2.7/config-x86_64-linux-gnu -lpython2.7 -lpthread -ldl -lutil -lm -Xlinker -export-dynamic -Wl,-O1 -Wl,-Bsymbolic-functions      
[34m[1miptables version[0m
iptables v1.4.21
[34m[1mcurl version[0m
curl 7.35.0 (x86_64-pc-linux-gnu) libcurl/7.35.0 OpenSSL/1.0.1f zlib/1.2.8 libidn/1.28 librtmp/2.3
[34m[1mwget version[0m
GNU Wget 1.15 built on linux-gnu.
[34m[1mrsync version[0m
rsync  version 3.1.0  protocol version 31
[34m[1mgimme version[0m
v1.2.0
[34m[1mnvm version[0m
0.33.6
[34m[1mperlbrew version[0m
/home/travis/perl5/perlbrew/bin/perlbrew  - App::perlbrew/0.80
[34m[1mphpenv version[0m
rbenv 1.1.1-25-g6aa70b6
[34m[1mrvm version[0m
rvm 1.29.3 (latest) by Michal Papis, Piotr Kuczynski, Wayne E. Seguin [https://rvm.io]
[34m[1mdefault ruby version[0m
ruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mElasticSearch version[0m
5.5.0
[34m[1mInstalled Firefox version[0m
firefox 56.0.2
[34m[1mMongoDB version[0m
MongoDB 3.4.10
[34m[1mPhantomJS version[0m
2.1.1
[34m[1mPre-installed PostgreSQL versions[0m
9.2.24
9.3.20
9.4.15
9.5.10
9.6.6
[34m[1mRabbitMQ Version[0m
3.6.14
[34m[1mRedis version[0m
redis-server 4.0.6
[34m[1mriak version[0m
2.2.3
[34m[1mPre-installed Go versions[0m
1.7.4
[34m[1mant version[0m
Apache Ant(TM) version 1.9.3 compiled on April 8 2014
[34m[1mmvn version[0m
Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T07:58:13Z)
Maven home: /usr/local/maven-3.5.2
Java version: 1.8.0_151, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "4.4.0-98-generic", arch: "amd64", family: "unix"
[34m[1mgradle version[0m

------------------------------------------------------------
Gradle 4.0.1
------------------------------------------------------------

Build time:   2017-07-07 14:02:41 UTC
Revision:     38e5dc0f772daecca1d2681885d3d85414eb6826

Groovy:       2.4.11
Ant:          Apache Ant(TM) version 1.9.6 compiled on June 29 2015
JVM:          1.8.0_151 (Oracle Corporation 25.151-b12)
OS:           Linux 4.4.0-98-generic amd64

[34m[1mlein version[0m
Leiningen 2.8.1 on Java 1.8.0_151 Java HotSpot(TM) 64-Bit Server VM
[34m[1mPre-installed Node.js versions[0m
v4.8.6
v6.12.0
v6.12.1
v8.9
v8.9.1
[34m[1mphpenv versions[0m
  system
  5.6
* 5.6.32 (set by /home/travis/.phpenv/version)
  7.0
  7.0.25
  7.1
  7.1.11
  hhvm
  hhvm-stable
[34m[1mcomposer --version[0m
Composer version 1.5.2 2017-09-11 16:59:25
[34m[1mPre-installed Ruby versions[0m
ruby-2.2.7
ruby-2.3.4
ruby-2.4.1
travis_fold:end:system_info
[0K

travis_fold:start:install_jdk
[0K[33;1mInstalling oraclejdk8[0m
$ jdk_switcher use "oraclejdk8"
Switching to Oracle JDK8 (java-8-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-8-oracle
travis_fold:end:install_jdk
[0Ktravis_fold:start:services
[0Ktravis_time:start:02f0d406
[0K$ sudo service docker start
start: Job is already running: docker
travis_time:end:02f0d406:start=1552951420434343969,finish=1552951420449550554,duration=15206585
[0Ktravis_fold:end:services
[0K
travis_fold:start:git.checkout
[0Ktravis_time:start:1a5fd5e3
[0K$ git clone --depth=50 --branch=refactor/renamed-a2m-mapper https://github.com/ExpediaDotCom/adaptive-alerting.git ExpediaDotCom/adaptive-alerting
Cloning into 'ExpediaDotCom/adaptive-alerting'...
travis_time:end:1a5fd5e3:start=1552951423457895221,finish=1552951424358388851,duration=900493630
[0K$ cd ExpediaDotCom/adaptive-alerting
$ git checkout -qf cdbd05eeaefc24394132e4c332a99f235fe13b00
travis_fold:end:git.checkout
[0K

[33;1mSetting environment variables from repository settings[0m
$ export DOCKER_PASSWORD=[secure]
$ export DOCKER_USERNAME=[secure]
$ export GPG_EXECUTABLE=gpg
$ export SONATYPE_USERNAME=expedia-haystack-admin
$ export SONATYPE_PASSWORD=[secure]
$ export GPG_SECRET_KEYS=[secure]
$ export GPG_PASSPHRASE=[secure]
$ export GPG_OWNERTRUST=[secure]

[33;1mSetting environment variables from .travis.yml[0m
$ export BRANCH=${TRAVIS_BRANCH}
$ export TAG=${TRAVIS_TAG}
$ export SHA=${TRAVIS_COMMIT}

travis_fold:start:cache.1
[0KSetting up build cache
$ export CASHER_DIR=${TRAVIS_HOME}/.casher
travis_time:start:0c2486b2
[0K$ Installing caching utilities
travis_time:end:0c2486b2:start=1552951428578926783,finish=1552951428746689701,duration=167762918
[0Ktravis_time:start:024f1350
[0Ktravis_time:end:024f1350:start=1552951428753093632,finish=1552951428756305017,duration=3211385
[0Ktravis_time:start:16b8b67a
[0K[32;1mattempting to download cache archive[0m
[32;1mfetching renamed-a2m-mapper/cache-linux-trusty-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855--jdk-oraclejdk8.tgz[0m
[32;1mfetching renamed-a2m-mapper/cache--jdk-oraclejdk8.tgz[0m
[32;1mfetching refactorrenamed-a2m-mapper/cache-linux-trusty-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855--jdk-oraclejdk8.tgz[0m
[32;1mfetching refactorrenamed-a2m-mapper/cache--jdk-oraclejdk8.tgz[0m
[32;1mfetching master/cache-linux-trusty-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855--jdk-oraclejdk8.tgz[0m
[32;1mfound cache[0m
travis_time:end:16b8b67a:start=1552951428761015993,finish=1552951436742265698,duration=7981249705
[0Ktravis_time:start:13b954e8
[0Ktravis_time:end:13b954e8:start=1552951436747813412,finish=1552951436751151717,duration=3338305
[0Ktravis_time:start:03d29174
[0K[32;1madding /home/travis/.m2 to cache[0m
travis_time:end:03d29174:start=1552951436756593696,finish=1552951448236653603,duration=11480059907
[0Ktravis_fold:end:cache.1
[0K
$ java -Xmx32m -version
java version "1.8.0_151"
Java(TM) SE Runtime Environment (build 1.8.0_151-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)
$ javac -J-Xmx32m -version
javac 1.8.0_151
travis_fold:start:install
[0Ktravis_time:start:01880560
[0K$ ./mvnw install -DskipTests=true -Dmaven.javadoc.skip=true -B -V
--2019-03-18 23:24:08--  https://repo.maven.apache.org/maven2/io/takari/maven-wrapper/0.4.2/maven-wrapper-0.4.2.jar
Resolving repo.maven.apache.org (repo.maven.apache.org)... 151.101.184.215
Connecting to repo.maven.apache.org (repo.maven.apache.org)|151.101.184.215|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 48337 (47K) [application/java-archive]
Saving to: œôòø/home/travis/build/ExpediaDotCom/adaptive-alerting/.mvn/wrapper/maven-wrapper.jarœôòù

     0K .......... .......... .......... .......... .......   100% 2.21M=0.02s

2019-03-18 23:24:08 (2.21 MB/s) - œôòø/home/travis/build/ExpediaDotCom/adaptive-alerting/.mvn/wrapper/maven-wrapper.jarœôòù saved [48337/48337]

Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z)
Maven home: /home/travis/.m2/wrapper/dists/apache-maven-3.5.4-bin/4lcg54ki11c6mp435njk296gm5/apache-maven-3.5.4
Java version: 1.8.0_151, vendor: Oracle Corporation, runtime: /usr/lib/jvm/java-8-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "4.4.0-101-generic", arch: "amd64", family: "unix"
[INFO] Scanning for projects...
[INFO] Inspecting build with total of 8 modules...
[INFO] Installing Nexus Staging features:
[INFO]   ... total of 8 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting                     [pom]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core                [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect          [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice        [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples             [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting           [pom]
[INFO] 
[INFO] -----------< com.expedia.adaptivealerting:adaptive-alerting >-----------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT [1/8]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting/1.0.0-SNAPSHOT/adaptive-alerting-1.0.0-SNAPSHOT.pom
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-core >---------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-core 1.0.0-SNAPSHOT [2/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-core ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-core ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 19 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-core ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 14 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-core ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-core ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-core ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/jacoco.exec
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-core ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-core ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-core ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-core ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-core ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] -----< com.expedia.adaptivealerting:adaptive-alerting-anomdetect >------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-anomdetect 1.0.0-SNAPSHOT [3/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-anomdetect ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-anomdetect ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-anomdetect ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 40 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/util/ModelResource.java: Some input files use or override a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/util/ModelResource.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSource.java: Some input files use unchecked or unsafe operations.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSource.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-anomdetect ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 15 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-anomdetect ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/test-classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/DefaultDetectorSourceTest.java: Some input files use or override a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/DefaultDetectorSourceTest.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSourceTest.java: Some input files use unchecked or unsafe operations.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSourceTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-anomdetect ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-anomdetect ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-anomdetect ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/jacoco.exec
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-anomdetect ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-anomdetect ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-anomdetect ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-anomdetect ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-anomdetect ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-kafka >--------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-kafka 1.0.0-SNAPSHOT [4/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] Downloading from apache-snapshots: https://repository.apache.org/snapshots/com/expedia/alertmanager/alert-manager-api/1.0.0-SNAPSHOT/maven-metadata.xml
[INFO] Downloading from apache-snapshots: https://repository.apache.org/snapshots/com/expedia/alertmanager/alert-manager/1.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-kafka ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-kafka ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 22 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java uses or overrides a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-kafka ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.4.2:testCompile (scala-test-compile) @ adaptive-alerting-kafka ---
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java:-1: info: compiling
[INFO] Compiling 17 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes at 1552951467350
[INFO] prepare-compile in 0 s
[INFO] compile in 3 s
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-kafka ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 17 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java uses or overrides a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-kafka ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalatest-maven-plugin:2.0.0:test (test) @ adaptive-alerting-kafka ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-kafka ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-kafka ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-kafka ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-kafka ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-kafka ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-assembly-plugin:3.1.0:single (default) @ adaptive-alerting-kafka ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT-jar-with-dependencies.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.22.1:integration-test (default) @ adaptive-alerting-kafka ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-kafka ---
[INFO] 
[INFO] --- maven-failsafe-plugin:2.22.1:verify (default) @ adaptive-alerting-kafka ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-kafka ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-kafka/1.0.0-SNAPSHOT/adaptive-alerting-kafka-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-kafka/1.0.0-SNAPSHOT/adaptive-alerting-kafka-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-kafka/1.0.0-SNAPSHOT/adaptive-alerting-kafka-1.0.0-SNAPSHOT-sources.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/adaptive-alerting-kafka-1.0.0-SNAPSHOT-jar-with-dependencies.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-kafka/1.0.0-SNAPSHOT/adaptive-alerting-kafka-1.0.0-SNAPSHOT-jar-with-dependencies.jar
[INFO] 
[INFO] ----< com.expedia.adaptivealerting:adaptive-alerting-modelservice >-----
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-modelservice 1.0.0-SNAPSHOT [5/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-modelservice ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-modelservice ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-modelservice ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 19 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-modelservice ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-modelservice ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 2 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/test-classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/src/test/java/com/expedia/adaptivealerting/modelservice/util/JpaConverterJsonTest.java: /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/src/test/java/com/expedia/adaptivealerting/modelservice/util/JpaConverterJsonTest.java uses unchecked or unsafe operations.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/src/test/java/com/expedia/adaptivealerting/modelservice/util/JpaConverterJsonTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-modelservice ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-modelservice ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-modelservice ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/jacoco.exec
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-modelservice ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/adaptive-alerting-modelservice-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-modelservice ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/adaptive-alerting-modelservice-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-modelservice ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- spring-boot-maven-plugin:2.0.5.RELEASE:repackage (default) @ adaptive-alerting-modelservice ---
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-modelservice ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-modelservice ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/adaptive-alerting-modelservice-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-modelservice/1.0.0-SNAPSHOT/adaptive-alerting-modelservice-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-modelservice/1.0.0-SNAPSHOT/adaptive-alerting-modelservice-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target/adaptive-alerting-modelservice-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-modelservice/1.0.0-SNAPSHOT/adaptive-alerting-modelservice-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-tools >--------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-tools 1.0.0-SNAPSHOT [6/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-tools ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-tools ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-tools ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 14 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-tools ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-tools ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 8 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-tools ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-tools ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-tools ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/jacoco.exec
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-tools ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/adaptive-alerting-tools-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-tools ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/adaptive-alerting-tools-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-tools ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-tools ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-tools ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/adaptive-alerting-tools-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-tools/1.0.0-SNAPSHOT/adaptive-alerting-tools-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-tools/1.0.0-SNAPSHOT/adaptive-alerting-tools-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target/adaptive-alerting-tools-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-tools/1.0.0-SNAPSHOT/adaptive-alerting-tools-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] -------< com.expedia.adaptivealerting:adaptive-alerting-samples >-------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-samples 1.0.0-SNAPSHOT [7/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-samples ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-samples ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 11 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-samples ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 0 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-samples ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-samples ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-samples ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target/adaptive-alerting-samples-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-samples ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target/adaptive-alerting-samples-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-samples ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-samples ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-samples ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target/adaptive-alerting-samples-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-samples/1.0.0-SNAPSHOT/adaptive-alerting-samples-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-samples/1.0.0-SNAPSHOT/adaptive-alerting-samples-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target/adaptive-alerting-samples-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-samples/1.0.0-SNAPSHOT/adaptive-alerting-samples-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] ------< com.expedia.adaptivealerting:adaptive-alerting-reporting >------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-reporting 1.0.0-SNAPSHOT [8/8]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-reporting ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/reporting/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-reporting ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-reporting ---
[INFO] Skipping JaCoCo execution due to missing execution data file:/home/travis/build/ExpediaDotCom/adaptive-alerting/reporting/target/jacoco.exec
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-reporting ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-reporting ---
[INFO] Skipping javadoc generation
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-reporting ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report-aggregate (report-aggregate) @ adaptive-alerting-reporting ---
[INFO] Analyzed bundle 'adaptive-alerting-anomdetect' with 34 classes
[INFO] Analyzed bundle 'adaptive-alerting-core' with 16 classes
[INFO] Analyzed bundle 'adaptive-alerting-kafka' with 30 classes
[INFO] Analyzed bundle 'adaptive-alerting-modelservice' with 6 classes
[INFO] Analyzed bundle 'adaptive-alerting-tools' with 10 classes
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-reporting ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/reporting/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-reporting/1.0.0-SNAPSHOT/adaptive-alerting-reporting-1.0.0-SNAPSHOT.pom
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT SUCCESS [  2.348 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core  SUCCESS [  5.458 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect SUCCESS [  4.178 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka SUCCESS [ 21.566 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice SUCCESS [  3.542 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools SUCCESS [  1.933 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples SUCCESS [  1.107 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting 1.0.0-SNAPSHOT SUCCESS [  0.875 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 42.632 s
[INFO] Finished at: 2019-03-18T23:24:53Z
[INFO] ------------------------------------------------------------------------
travis_time:end:01880560:start=1552951448865694427,finish=1552951493330939580,duration=44465245153
[0Ktravis_fold:end:install
[0Ktravis_time:start:148f69ff
[0K$ if ([ "$TRAVIS_BRANCH" == "master" ] && [ "$TRAVIS_PULL_REQUEST" == "false" ]) || [ -n "$TRAVIS_TAG" ]; then make release; else make all; fi
./mvnw clean
[INFO] Scanning for projects...
[INFO] Inspecting build with total of 8 modules...
[INFO] Installing Nexus Staging features:
[INFO]   ... total of 8 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting                     [pom]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core                [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect          [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice        [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples             [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting           [pom]
[INFO] 
[INFO] -----------< com.expedia.adaptivealerting:adaptive-alerting >-----------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT [1/8]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting ---
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-core >---------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-core 1.0.0-SNAPSHOT [2/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-core ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target
[INFO] 
[INFO] -----< com.expedia.adaptivealerting:adaptive-alerting-anomdetect >------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-anomdetect 1.0.0-SNAPSHOT [3/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-anomdetect ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-kafka >--------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-kafka 1.0.0-SNAPSHOT [4/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-kafka ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target
[INFO] 
[INFO] ----< com.expedia.adaptivealerting:adaptive-alerting-modelservice >-----
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-modelservice 1.0.0-SNAPSHOT [5/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-modelservice ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/modelservice/target
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-tools >--------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-tools 1.0.0-SNAPSHOT [6/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-tools ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/tools/target
[INFO] 
[INFO] -------< com.expedia.adaptivealerting:adaptive-alerting-samples >-------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-samples 1.0.0-SNAPSHOT [7/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-samples ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/samples/target
[INFO] 
[INFO] ------< com.expedia.adaptivealerting:adaptive-alerting-reporting >------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-reporting 1.0.0-SNAPSHOT [8/8]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ adaptive-alerting-reporting ---
[INFO] Deleting /home/travis/build/ExpediaDotCom/adaptive-alerting/reporting/target
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT SUCCESS [  0.217 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core  SUCCESS [  0.040 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect SUCCESS [  0.031 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka SUCCESS [  0.068 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice SUCCESS [  0.087 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools SUCCESS [  0.040 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples SUCCESS [  0.038 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting 1.0.0-SNAPSHOT SUCCESS [  0.044 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.222 s
[INFO] Finished at: 2019-03-18T23:24:56Z
[INFO] ------------------------------------------------------------------------
./mvnw install
[INFO] Scanning for projects...
[INFO] Inspecting build with total of 8 modules...
[INFO] Installing Nexus Staging features:
[INFO]   ... total of 8 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting                     [pom]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core                [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect          [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice        [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools               [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples             [jar]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting           [pom]
[INFO] 
[INFO] -----------< com.expedia.adaptivealerting:adaptive-alerting >-----------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT [1/8]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting ---
[INFO] Not executing Javadoc as the project is not a Java classpath-capable package
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting/1.0.0-SNAPSHOT/adaptive-alerting-1.0.0-SNAPSHOT.pom
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-core >---------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-core 1.0.0-SNAPSHOT [2/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-core ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-core ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 19 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-core ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-core ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 14 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-core ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.expedia.adaptivealerting.core.anomaly.AnomalyThresholdsTest
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.214 s - in com.expedia.adaptivealerting.core.anomaly.AnomalyThresholdsTest
[INFO] Running com.expedia.adaptivealerting.core.anomaly.AnomalyResultTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.expedia.adaptivealerting.core.anomaly.AnomalyResultTest
[INFO] Running com.expedia.adaptivealerting.core.anomaly.StringToAnomalyLevelConverterTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.expedia.adaptivealerting.core.anomaly.StringToAnomalyLevelConverterTest
[INFO] Running com.expedia.adaptivealerting.core.util.ReflectionUtilTest
2019-03-18 23:25:07 ERROR ReflectionUtil:36 - Error instantiating com.expedia.adaptivealerting.core.anomaly.AnomalyThresholds
java.lang.InstantiationException: com.expedia.adaptivealerting.core.anomaly.AnomalyThresholds
	at java.lang.Class.newInstance(Class.java:427)
	at com.expedia.adaptivealerting.core.util.ReflectionUtil.newInstance(ReflectionUtil.java:34)
	at com.expedia.adaptivealerting.core.util.ReflectionUtilTest.testNewInstance_instantiationException(ReflectionUtilTest.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.lang.NoSuchMethodException: com.expedia.adaptivealerting.core.anomaly.AnomalyThresholds.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.newInstance(Class.java:412)
	... 28 more
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.16 s - in com.expedia.adaptivealerting.core.util.ReflectionUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.MetricUtilTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.expedia.adaptivealerting.core.util.MetricUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.MathUtilTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.expedia.adaptivealerting.core.util.MathUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.ThreadUtilTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.431 s - in com.expedia.adaptivealerting.core.util.ThreadUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.DateUtilTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.expedia.adaptivealerting.core.util.DateUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.AssertUtilTest
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 s - in com.expedia.adaptivealerting.core.util.AssertUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.ErrorUtilTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.expedia.adaptivealerting.core.util.ErrorUtilTest
[INFO] Running com.expedia.adaptivealerting.core.util.ObjectMapperTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.63 s - in com.expedia.adaptivealerting.core.util.ObjectMapperTest
[INFO] Running com.expedia.adaptivealerting.core.data.MetricFrameLoaderTest
2019-03-18 23:25:08 INFO  MetricFrameLoaderTest:45 - defFile=/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/test-classes/datasets/cal-inflow-metric-def.json, dataFile=/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/test-classes/datasets/cal-inflow.csv
2019-03-18 23:25:09 INFO  MetricFrameLoader:43 - metricDef=MetricDefinition{key='cal-inflow', tags=TagCollection{kv={what=traffic}, v=[]}, meta=TagCollection{kv={}, v=[]}}
2019-03-18 23:25:09 INFO  MetricFrameLoader:44 - metricId=1.ff428b9daaf53ff0984e12b9c6c03017
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.676 s - in com.expedia.adaptivealerting.core.data.MetricFrameLoaderTest
[INFO] Running com.expedia.adaptivealerting.core.evaluator.RmseEvaluatorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.213 s - in com.expedia.adaptivealerting.core.evaluator.RmseEvaluatorTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 39, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-core ---
[INFO] Loading execution data file /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/jacoco.exec
[INFO] Analyzed bundle 'com.expedia.adaptivealerting:adaptive-alerting-core' with 16 classes
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-core ---
[INFO] Loading execution data file /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/jacoco.exec
[INFO] Analyzed bundle 'adaptive-alerting-core' with 16 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-core ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-core ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-core ---
[INFO] 
Loading source files for package com.expedia.adaptivealerting.core.anomaly...
Loading source files for package com.expedia.adaptivealerting.core.util...
Loading source files for package com.expedia.adaptivealerting.core.data...
Loading source files for package com.expedia.adaptivealerting.core.evaluator...
Constructing Javadoc information...
Standard Doclet version 1.8.0_151
Building tree for all the packages and classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/AnomalyLevel.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/AnomalyResult.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/AnomalyThresholds.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/AnomalyType.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/StringToAnomalyLevelConverter.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/AssertUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/DateUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/ErrorUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/MathUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/MetricUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/ObjectMapperUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/ReflectionUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/ThreadUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/MappedMetricData.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/MetricFrame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/MetricFrameLoader.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/Evaluator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/ModelEvaluation.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/RmseEvaluator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/overview-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/constant-values.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/class-use/AnomalyType.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/class-use/StringToAnomalyLevelConverter.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/class-use/AnomalyLevel.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/class-use/AnomalyResult.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/class-use/AnomalyThresholds.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/ThreadUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/MathUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/MetricUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/ReflectionUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/ErrorUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/DateUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/AssertUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/class-use/ObjectMapperUtil.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/class-use/MetricFrameLoader.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/class-use/MappedMetricData.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/class-use/MetricFrame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/class-use/RmseEvaluator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/class-use/ModelEvaluation.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/class-use/Evaluator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/anomaly/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/data/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/evaluator/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/com/expedia/adaptivealerting/core/util/package-use.html...
Building index for all the packages and classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/overview-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/index-all.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/allclasses-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/allclasses-noframe.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/index.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/overview-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/apidocs/help-doc.html...
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-core ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-core ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT-sources.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT-javadoc.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-core/1.0.0-SNAPSHOT/adaptive-alerting-core-1.0.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] -----< com.expedia.adaptivealerting:adaptive-alerting-anomdetect >------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-anomdetect 1.0.0-SNAPSHOT [3/8]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-anomdetect ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-anomdetect ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-anomdetect ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 40 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/util/ModelResource.java: Some input files use or override a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/util/ModelResource.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSource.java: Some input files use unchecked or unsafe operations.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSource.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-anomdetect ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 15 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-anomdetect ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 26 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/test-classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/DefaultDetectorSourceTest.java: Some input files use or override a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/DefaultDetectorSourceTest.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSourceTest.java: Some input files use unchecked or unsafe operations.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/test/java/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSourceTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-anomdetect ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.expedia.adaptivealerting.anomdetect.AnomalyToMetricMapperTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.174 s - in com.expedia.adaptivealerting.anomdetect.AnomalyToMetricMapperTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.DetectorLookupTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 s - in com.expedia.adaptivealerting.anomdetect.DetectorLookupTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.source.TempHaystackAwareDetectorSourceTest
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:101 - Different metrics have different UUIDs:
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:102 -   uuid1=15b983ab-378d-3611-a263-5c6bc3f8e819
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:103 -   uuid2=1d75d379-b788-4b38-b4c2-83bdf14eb279
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:83 - Same metric has same UUID:
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:84 -   uuid1=15b983ab-378d-3611-a263-5c6bc3f8e819
2019-03-18 23:25:20 INFO  TempHaystackAwareDetectorSourceTest:85 -   uuid2=15b983ab-378d-3611-a263-5c6bc3f8e819
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.478 s - in com.expedia.adaptivealerting.anomdetect.source.TempHaystackAwareDetectorSourceTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.source.DefaultDetectorSourceTest
2019-03-18 23:25:21 INFO  DefaultDetectorSource:89 - Found detector: EwmaAnomalyDetector(mean=0.0, variance=0.0)
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.878 s - in com.expedia.adaptivealerting.anomdetect.source.DefaultDetectorSourceTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.util.ModelServiceConnectorTest
[INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.74 s - in com.expedia.adaptivealerting.anomdetect.util.ModelServiceConnectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.DetectorMapperTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 s - in com.expedia.adaptivealerting.anomdetect.DetectorMapperTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.DetectorManagerTest
2019-03-18 23:25:22 WARN  DetectorManager:79 - No detector for mappedMetricData=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='bad-definition', tags=TagCollection{kv={}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951522}, detectorUuid=ba632071-c977-4f3a-bc7e-f301c2580ed9, anomalyResult=null)
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.119 s - in com.expedia.adaptivealerting.anomdetect.DetectorManagerTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.individuals.IndividualsChartAnomalyDetectorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.941 s - in com.expedia.adaptivealerting.anomdetect.individuals.IndividualsChartAnomalyDetectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.cusum.CusumAnomalyDetectorTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.138 s - in com.expedia.adaptivealerting.anomdetect.cusum.CusumAnomalyDetectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.constant.ConstantThresholdAnomalyDetectorTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 s - in com.expedia.adaptivealerting.anomdetect.constant.ConstantThresholdAnomalyDetectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.pewma.PewmaAnomalyDetectorTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.27 s - in com.expedia.adaptivealerting.anomdetect.pewma.PewmaAnomalyDetectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersAnomalyDetectorTest
2019-03-18 23:25:29 WARN  HoltWintersParams:151 - warmUpPeriod (4) should be greater than or equal to (frequency * 2) (8), as the detector will not emit anomalies during training. Setting warmUpPeriod to 8.
2019-03-18 23:25:29 WARN  HoltWintersParams:151 - warmUpPeriod (4) should be greater than or equal to (frequency * 2) (8), as the detector will not emit anomalies during training. Setting warmUpPeriod to 8.
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.473 s - in com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersAnomalyDetectorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersParamsTest
2019-03-18 23:25:29 WARN  HoltWintersParams:151 - warmUpPeriod (0) should be greater than or equal to (frequency * 2) (8), as the detector will not emit anomalies during training. Setting warmUpPeriod to 8.
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.036 s - in com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersParamsTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersSeasonalEstimatesValidatorTest
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersSeasonalEstimatesValidatorTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersOnlineComponentsTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 s - in com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersOnlineComponentsTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersSimpleTrainingModelTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 s - in com.expedia.adaptivealerting.anomdetect.holtwinters.HoltWintersSimpleTrainingModelTest
[INFO] Running com.expedia.adaptivealerting.anomdetect.ewma.EwmaAnomalyDetectorTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.496 s - in com.expedia.adaptivealerting.anomdetect.ewma.EwmaAnomalyDetectorTest
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 93, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:report (jacoco-report) @ adaptive-alerting-anomdetect ---
[INFO] Loading execution data file /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/jacoco.exec
[INFO] Analyzed bundle 'com.expedia.adaptivealerting:adaptive-alerting-anomdetect' with 34 classes
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:check (jacoco-check) @ adaptive-alerting-anomdetect ---
[INFO] Loading execution data file /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/jacoco.exec
[INFO] Analyzed bundle 'adaptive-alerting-anomdetect' with 34 classes
[INFO] All coverage checks have been met.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ adaptive-alerting-anomdetect ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar-no-fork (attach-sources) @ adaptive-alerting-anomdetect ---
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.0.1:jar (attach-javadocs) @ adaptive-alerting-anomdetect ---
[INFO] 
Loading source files for package com.expedia.adaptivealerting.anomdetect...
Loading source files for package com.expedia.adaptivealerting.anomdetect.source...
Loading source files for package com.expedia.adaptivealerting.anomdetect.util...
Loading source files for package com.expedia.adaptivealerting.anomdetect.individuals...
Loading source files for package com.expedia.adaptivealerting.anomdetect.cusum...
Loading source files for package com.expedia.adaptivealerting.anomdetect.constant...
Loading source files for package com.expedia.adaptivealerting.anomdetect.pewma...
Loading source files for package com.expedia.adaptivealerting.anomdetect.holtwinters...
Loading source files for package com.expedia.adaptivealerting.anomdetect.ewma...
Constructing Javadoc information...
Standard Doclet version 1.8.0_151
Building tree for all the packages and classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/AbstractAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/AnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/AnomalyToMetricMapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/DetectorLookup.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/DetectorManager.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/DetectorMapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/DetectorParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/DefaultDetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/DetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/TempHaystackAwareDetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorDeserializationException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorNotFoundException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorResources.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorResources.Embedded.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/DetectorRetrievalException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/HttpClientWrapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/ModelResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/ModelResources.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/ModelResources.Embedded.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/ModelServiceConnector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/ModelTypeResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/IndividualsControlChartAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/IndividualsControlChartParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/CusumAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/CusumParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/ConstantThresholdAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/ConstantThresholdParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/PewmaAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/PewmaParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersClassificationException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersOnlineAlgorithm.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersOnlineComponents.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersSeasonalEstimatesValidator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersSimpleTrainingModel.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersTrainingMethod.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/SeasonalityType.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/EwmaAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/EwmaParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/overview-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/package-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/package-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/package-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/constant-values.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/serialized-form.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/DetectorLookup.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/AnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/AbstractAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/DetectorParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/DetectorMapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/AnomalyToMetricMapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/class-use/DetectorManager.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/class-use/DetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/class-use/DefaultDetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/class-use/TempHaystackAwareDetectorSource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorDeserializationException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/ModelResources.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/ModelResources.Embedded.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorRetrievalException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/HttpClientWrapper.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/ModelTypeResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/ModelResource.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorNotFoundException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/ModelServiceConnector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorResources.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/class-use/DetectorResources.Embedded.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/class-use/IndividualsControlChartParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/class-use/IndividualsControlChartAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/class-use/CusumParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/class-use/CusumAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/class-use/ConstantThresholdAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/class-use/ConstantThresholdParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/class-use/PewmaAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/class-use/PewmaParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersOnlineAlgorithm.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersClassificationException.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersSeasonalEstimatesValidator.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersTrainingMethod.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersSimpleTrainingModel.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/SeasonalityType.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/class-use/HoltWintersOnlineComponents.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/class-use/EwmaParams.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/class-use/EwmaAnomalyDetector.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/constant/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/cusum/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/ewma/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/holtwinters/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/individuals/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/pewma/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/source/package-use.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/com/expedia/adaptivealerting/anomdetect/util/package-use.html...
Building index for all the packages and classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/overview-tree.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/index-all.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/allclasses-frame.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/allclasses-noframe.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/index.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/overview-summary.html...
Generating /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/apidocs/help-doc.html...
2 warnings
[WARNING] Javadoc Warnings
[WARNING] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersOnlineComponents.java:33: warning - Tag @see: missing final '>': "<a href="https://otexts.org/fpp2/holt-winters.html">Holt-Winters' Seasonal Method</a>
[WARNING] and <a href="https://robjhyndman.com/hyndsight/seasonal-periods/">https://robjhyndman.com/hyndsight/seasonal-periods/</a> for naming conventions
[WARNING] (e.g. usage of "frequency" and "cycle")."
[WARNING] /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/src/main/java/com/expedia/adaptivealerting/anomdetect/holtwinters/HoltWintersParams.java:37: warning - Tag @see: missing final '>': "<a href="https://otexts.org/fpp2/holt-winters.html">Holt-Winters' Seasonal Method</a>
[WARNING] and <a href="https://robjhyndman.com/hyndsight/seasonal-periods/">https://robjhyndman.com/hyndsight/seasonal-periods/</a> for naming conventions
[WARNING] (e.g. usage of "frequency" and "cycle")."
[INFO] Building jar: /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-gpg-plugin:1.6:sign (sign-artifacts) @ adaptive-alerting-anomdetect ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ adaptive-alerting-anomdetect ---
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/pom.xml to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.pom
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar
[INFO] Installing /home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-javadoc.jar to /home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --------< com.expedia.adaptivealerting:adaptive-alerting-kafka >--------
[INFO] Building com.expedia.adaptivealerting:adaptive-alerting-kafka 1.0.0-SNAPSHOT [4/8]
[INFO] --------------------------------[ jar ]---------------------------------
Downloading from apache-snapshots: https://repository.apache.org/snapshots/com/expedia/alertmanager/alert-manager-api/1.0.0-SNAPSHOT/maven-metadata.xml
Downloading from apache-snapshots: https://repository.apache.org/snapshots/com/expedia/alertmanager/alert-manager/1.0.0-SNAPSHOT/maven-metadata.xml
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.3:prepare-agent (jacoco-initialize) @ adaptive-alerting-kafka ---
[INFO] argLine set to -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.8.3/org.jacoco.agent-0.8.3-runtime.jar=destfile=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ adaptive-alerting-kafka ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ adaptive-alerting-kafka ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 22 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java uses or overrides a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/main/java/com/expedia/adaptivealerting/kafka/util/DetectorUtil.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ adaptive-alerting-kafka ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.4.2:testCompile (scala-test-compile) @ adaptive-alerting-kafka ---
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java:-1: info: compiling
[INFO] Compiling 17 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes at 1552951542047
[INFO] prepare-compile in 0 s
[INFO] compile in 3 s
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ adaptive-alerting-kafka ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 17 source files to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java: /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java uses or overrides a deprecated API.
[INFO] /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/src/test/java/com.expedia.adaptivealerting.kafka/notifier/NotifierTest.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ adaptive-alerting-kafka ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.expedia.adaptivealerting.kafka.KafkaNotifierTest
2019-03-18 23:25:48 INFO  AbstractContextLoader:264 - Could not detect default resource locations for test class [com.expedia.adaptivealerting.kafka.KafkaNotifierTest]: no resource found for suffixes {-context.xml, Context.groovy}.
2019-03-18 23:25:48 INFO  SpringBootTestContextBootstrapper:248 - Loaded default TestExecutionListener class names from location [META-INF/spring.factories]: [org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener, org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener, org.springframework.test.context.web.ServletTestExecutionListener, org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener, org.springframework.test.context.support.DependencyInjectionTestExecutionListener, org.springframework.test.context.support.DirtiesContextTestExecutionListener, org.springframework.test.context.transaction.TransactionalTestExecutionListener, org.springframework.test.context.jdbc.SqlScriptsTestExecutionListener]
2019-03-18 23:25:48 INFO  SpringBootTestContextBootstrapper:177 - Using TestExecutionListeners: [org.springframework.test.context.web.ServletTestExecutionListener@5bda157e, org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener@67e0fd6d, org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener@21390938, org.springframework.boot.test.autoconfigure.SpringBootDependencyInjectionTestExecutionListener@1129829c, org.springframework.test.context.support.DirtiesContextTestExecutionListener@1a531422, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener@7a388990, org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener@13213f26, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener@4e4162bc, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener@4c319d52, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener@72fbf94d]
Mar 18, 2019 11:25:49 PM okhttp3.mockwebserver.MockWebServer$2 execute
INFO: MockWebServer[51515] starting to accept connections
2019-03-18 23:25:49 INFO  QuorumPeerConfig:327 - clientPortAddress is 0.0.0.0/0.0.0.0:45726
2019-03-18 23:25:49 INFO  QuorumPeerConfig:331 - secureClientPort is not set
2019-03-18 23:25:49 INFO  TestingZooKeeperMain:218 - Starting server
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:zookeeper.version=3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60, built on 04/03/2017 16:19 GMT
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:host.name=travis-job-94c84eda-d6a3-408d-97cc-f0c7c6a94056
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.version=1.8.0_151
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.vendor=Oracle Corporation
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.home=/usr/lib/jvm/java-8-oracle/jre
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.class.path=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes:/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/classes:/home/travis/.m2/repository/com/codahale/metrics/metrics-core/3.0.2/metrics-core-3.0.2.jar:/home/travis/.m2/repository/com/expedia/metrics-java/0.11.0/metrics-java-0.11.0.jar:/home/travis/.m2/repository/com/expedia/metrics-java-jackson/0.11.0/metrics-java-jackson-0.11.0.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.8/jackson-databind-2.9.8.jar:/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.8/jackson-annotations-2.9.8.jar:/home/travis/.m2/repository/com/opencsv/opencsv/4.1/opencsv-4.1.jar:/home/travis/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/travis/.m2/repository/org/apache/commons/commons-text/1.1/commons-text-1.1.jar:/home/travis/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/travis/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/travis/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.8/jackson-core-2.9.8.jar:/home/travis/.m2/repository/org/apache/httpcomponents/fluent-hc/4.5.6/fluent-hc-4.5.6.jar:/home/travis/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/travis/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/travis/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/home/travis/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams/1.1.1/kafka-streams-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-clients/1.1.1/kafka-clients-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/connect-json/1.1.1/connect-json-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/connect-api/1.1.1/connect-api-1.1.1.jar:/home/travis/.m2/repository/org/rocksdb/rocksdbjni/5.7.3/rocksdbjni-5.7.3.jar:/home/travis/.m2/repository/com/expedia/metrics-java-metrictank/0.11.0/metrics-java-metrictank-0.11.0.jar:/home/travis/.m2/repository/org/msgpack/msgpack-core/0.8.13/msgpack-core-0.8.13.jar:/home/travis/.m2/repository/com/expedia/alertmanager/alert-manager-api/1.0.0-SNAPSHOT/alert-manager-api-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter/2.0.5.RELEASE/spring-boot-starter-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot/2.0.5.RELEASE/spring-boot-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-context/5.0.9.RELEASE/spring-context-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/2.0.5.RELEASE/spring-boot-autoconfigure-2.0.5.RELEASE.jar:/home/travis/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/travis/.m2/repository/org/springframework/spring-core/5.0.9.RELEASE/spring-core-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-jcl/5.0.9.RELEASE/spring-jcl-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/yaml/snakeyaml/1.19/snakeyaml-1.19.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-web/2.0.5.RELEASE/spring-boot-starter-web-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-json/2.0.5.RELEASE/spring-boot-starter-json-2.0.5.RELEASE.jar:/home/travis/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.9.6/jackson-datatype-jdk8-2.9.6.jar:/home/travis/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.9.6/jackson-datatype-jsr310-2.9.6.jar:/home/travis/.m2/repository/com/fasterxml/jackson/module/jackson-module-parameter-names/2.9.6/jackson-module-parameter-names-2.9.6.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/2.0.5.RELEASE/spring-boot-starter-tomcat-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.34/tomcat-embed-core-8.5.34.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/8.5.34/tomcat-embed-el-8.5.34.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/8.5.34/tomcat-embed-websocket-8.5.34.jar:/home/travis/.m2/repository/org/hibernate/validator/hibernate-validator/6.0.12.Final/hibernate-validator-6.0.12.Final.jar:/home/travis/.m2/repository/javax/validation/validation-api/2.0.1.Final/validation-api-2.0.1.Final.jar:/home/travis/.m2/repository/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.jar:/home/travis/.m2/repository/com/fasterxml/classmate/1.3.4/classmate-1.3.4.jar:/home/travis/.m2/repository/org/springframework/spring-web/5.0.9.RELEASE/spring-web-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-beans/5.0.9.RELEASE/spring-beans-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-webmvc/5.0.9.RELEASE/spring-webmvc-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-aop/5.0.9.RELEASE/spring-aop-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-expression/5.0.9.RELEASE/spring-expression-5.0.9.RELEASE.jar:/home/travis/.m2/repository/com/google/guava/guava/23.3-jre/guava-23.3-jre.jar:/home/travis/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/travis/.m2/repository/com/google/errorprone/error_prone_annotations/2.0.18/error_prone_annotations-2.0.18.jar:/home/travis/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/travis/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/travis/.m2/repository/com/github/charithe/kafka-junit/4.1.1/kafka-junit-4.1.1.jar:/home/travis/.m2/repository/org/apache/curator/curator-test/4.0.0/curator-test-4.0.0.jar:/home/travis/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.1.1/junit-jupiter-api-5.1.1.jar:/home/travis/.m2/repository/org/apiguardian/apiguardian-api/1.0.0/apiguardian-api-1.0.0.jar:/home/travis/.m2/repository/org/opentest4j/opentest4j/1.0.0/opentest4j-1.0.0.jar:/home/travis/.m2/repository/org/junit/platform/junit-platform-commons/1.1.1/junit-platform-commons-1.1.1.jar:/home/travis/.m2/repository/com/squareup/okhttp3/mockwebserver/3.11.0/mockwebserver-3.11.0.jar:/home/travis/.m2/repository/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar:/home/travis/.m2/repository/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar:/home/travis/.m2/repository/org/apache/kafka/kafka_2.12/1.1.1/kafka_2.12-1.1.1.jar:/home/travis/.m2/repository/net/sf/jopt-simple/jopt-simple/4.9/jopt-simple-4.9.jar:/home/travis/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/travis/.m2/repository/org/scala-lang/scala-library/2.12.7/scala-library-2.12.7.jar:/home/travis/.m2/repository/org/scala-lang/scala-reflect/2.12.7/scala-reflect-2.12.7.jar:/home/travis/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.8.0/scala-logging_2.12-3.8.0.jar:/home/travis/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/travis/.m2/repository/org/apache/zookeeper/zookeeper/3.5.3-beta/zookeeper-3.5.3-beta.jar:/home/travis/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/travis/.m2/repository/org/apache/kafka/kafka_2.12/1.1.1/kafka_2.12-1.1.1-test.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-clients/1.1.1/kafka-clients-1.1.1-test.jar:/home/travis/.m2/repository/org/lz4/lz4-java/1.4.1/lz4-java-1.4.1.jar:/home/travis/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.1/snappy-java-1.1.7.1.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams/1.1.1/kafka-streams-1.1.1-test.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams-test-utils/1.1.1/kafka-streams-test-utils-1.1.1.jar:/home/travis/.m2/repository/org/scalatest/scalatest_2.12/3.0.5/scalatest_2.12-3.0.5.jar:/home/travis/.m2/repository/org/scalactic/scalactic_2.12/3.0.5/scalactic_2.12-3.0.5.jar:/home/travis/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-test/2.0.5.RELEASE/spring-boot-starter-test-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-test/2.0.5.RELEASE/spring-boot-test-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-test-autoconfigure/2.0.5.RELEASE/spring-boot-test-autoconfigure-2.0.5.RELEASE.jar:/home/travis/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/travis/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/travis/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/travis/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/travis/.m2/repository/org/assertj/assertj-core/3.9.1/assertj-core-3.9.1.jar:/home/travis/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/travis/.m2/repository/org/hamcrest/hamcrest-library/1.3/hamcrest-library-1.3.jar:/home/travis/.m2/repository/org/skyscreamer/jsonassert/1.5.0/jsonassert-1.5.0.jar:/home/travis/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar:/home/travis/.m2/repository/org/springframework/spring-test/5.0.9.RELEASE/spring-test-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/xmlunit/xmlunit-core/2.5.1/xmlunit-core-2.5.1.jar:/home/travis/.m2/repository/org/projectlombok/lombok/1.18.2/lombok-1.18.2.jar:/home/travis/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/travis/.m2/repository/io/micrometer/micrometer-registry-jmx/1.1.3/micrometer-registry-jmx-1.1.3.jar:/home/travis/.m2/repository/io/micrometer/micrometer-core/1.1.3/micrometer-core-1.1.3.jar:/home/travis/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.9/HdrHistogram-2.1.9.jar:/home/travis/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/home/travis/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.0.3/metrics-jmx-4.0.3.jar:/home/travis/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/travis/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/travis/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/travis/.m2/repository/org/mockito/mockito-core/2.23.0/mockito-core-2.23.0.jar:/home/travis/.m2/repository/net/bytebuddy/byte-buddy/1.7.11/byte-buddy-1.7.11.jar:/home/travis/.m2/repository/net/bytebuddy/byte-buddy-agent/1.7.11/byte-buddy-agent-1.7.11.jar:/home/travis/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.io.tmpdir=/tmp
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:java.compiler=<NA>
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.name=Linux
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.arch=amd64
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.version=4.4.0-101-generic
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:user.name=travis
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:user.home=/home/travis
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:user.dir=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.memory.free=182MB
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.memory.max=1662MB
2019-03-18 23:25:49 INFO  ZooKeeperServer:109 - Server environment:os.memory.total=203MB
2019-03-18 23:25:49 INFO  ZooKeeperServer:907 - minSessionTimeout set to 6000
2019-03-18 23:25:49 INFO  ZooKeeperServer:916 - maxSessionTimeout set to 60000
2019-03-18 23:25:49 INFO  ZooKeeperServer:159 - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/1552951549196-0/version-2 snapdir /tmp/1552951549196-0/version-2
2019-03-18 23:25:49 INFO  NIOServerCnxnFactory:673 - Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 4 worker threads, and 64 kB direct buffers.
2019-03-18 23:25:49 INFO  NIOServerCnxnFactory:686 - binding to port 0.0.0.0/0.0.0.0:45726
2019-03-18 23:25:49 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951549196-0/version-2/snapshot.0
2019-03-18 23:25:49 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951549196-0/version-2/snapshot.0
2019-03-18 23:25:49 INFO  ContainerManager:64 - Using checkIntervalMs=60000 maxPerMinute=10000
2019-03-18 23:25:50 INFO  Log4jControllerRegistration$:31 - Registered kafka:type=kafka.Log4jController MBean
2019-03-18 23:25:51 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:43808
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:43808
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit7191447525342995375
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 43808
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:45726
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:25:51 INFO  EphemeralKafkaBroker:144 - Starting Kafka server with config: {offsets.topic.replication.factor=1, advertised.listeners=PLAINTEXT://localhost:43808, default.replication.factor=1, transaction.state.log.replication.factor=1, offsets.topic.num.partitions=1, port=43808, transaction.state.log.min.isr=1, log.dirs=/tmp/kafka_junit7191447525342995375, group.min.session.timeout.ms=100, transaction.state.log.num.partitions=1, zookeeper.connect=127.0.0.1:45726, transaction.timeout.ms=500, leader.imbalance.check.interval.seconds=1, broker.id=1, num.partitions=1, listeners=PLAINTEXT://0.0.0.0:43808}
2019-03-18 23:25:51 INFO  KafkaServer:66 - starting
2019-03-18 23:25:51 INFO  KafkaServer:66 - Connecting to zookeeper on 127.0.0.1:45726
2019-03-18 23:25:51 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Initializing a new session to 127.0.0.1:45726.
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:zookeeper.version=3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60, built on 04/03/2017 16:19 GMT
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:host.name=travis-job-94c84eda-d6a3-408d-97cc-f0c7c6a94056
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.version=1.8.0_151
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.vendor=Oracle Corporation
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.class.path=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/test-classes:/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/classes:/home/travis/.m2/repository/com/codahale/metrics/metrics-core/3.0.2/metrics-core-3.0.2.jar:/home/travis/.m2/repository/com/expedia/metrics-java/0.11.0/metrics-java-0.11.0.jar:/home/travis/.m2/repository/com/expedia/metrics-java-jackson/0.11.0/metrics-java-jackson-0.11.0.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.8/jackson-databind-2.9.8.jar:/home/travis/build/ExpediaDotCom/adaptive-alerting/core/target/adaptive-alerting-core-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.8/jackson-annotations-2.9.8.jar:/home/travis/.m2/repository/com/opencsv/opencsv/4.1/opencsv-4.1.jar:/home/travis/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar:/home/travis/.m2/repository/org/apache/commons/commons-text/1.1/commons-text-1.1.jar:/home/travis/.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar:/home/travis/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/travis/.m2/repository/com/typesafe/config/1.3.3/config-1.3.3.jar:/home/travis/build/ExpediaDotCom/adaptive-alerting/anomdetect/target/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/travis/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.8/jackson-core-2.9.8.jar:/home/travis/.m2/repository/org/apache/httpcomponents/fluent-hc/4.5.6/fluent-hc-4.5.6.jar:/home/travis/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar:/home/travis/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar:/home/travis/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar:/home/travis/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams/1.1.1/kafka-streams-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-clients/1.1.1/kafka-clients-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/connect-json/1.1.1/connect-json-1.1.1.jar:/home/travis/.m2/repository/org/apache/kafka/connect-api/1.1.1/connect-api-1.1.1.jar:/home/travis/.m2/repository/org/rocksdb/rocksdbjni/5.7.3/rocksdbjni-5.7.3.jar:/home/travis/.m2/repository/com/expedia/metrics-java-metrictank/0.11.0/metrics-java-metrictank-0.11.0.jar:/home/travis/.m2/repository/org/msgpack/msgpack-core/0.8.13/msgpack-core-0.8.13.jar:/home/travis/.m2/repository/com/expedia/alertmanager/alert-manager-api/1.0.0-SNAPSHOT/alert-manager-api-1.0.0-SNAPSHOT.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter/2.0.5.RELEASE/spring-boot-starter-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot/2.0.5.RELEASE/spring-boot-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-context/5.0.9.RELEASE/spring-context-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/2.0.5.RELEASE/spring-boot-autoconfigure-2.0.5.RELEASE.jar:/home/travis/.m2/repository/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/travis/.m2/repository/org/springframework/spring-core/5.0.9.RELEASE/spring-core-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-jcl/5.0.9.RELEASE/spring-jcl-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/yaml/snakeyaml/1.19/snakeyaml-1.19.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-web/2.0.5.RELEASE/spring-boot-starter-web-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-json/2.0.5.RELEASE/spring-boot-starter-json-2.0.5.RELEASE.jar:/home/travis/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.9.6/jackson-datatype-jdk8-2.9.6.jar:/home/travis/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.9.6/jackson-datatype-jsr310-2.9.6.jar:/home/travis/.m2/repository/com/fasterxml/jackson/module/jackson-module-parameter-names/2.9.6/jackson-module-parameter-names-2.9.6.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/2.0.5.RELEASE/spring-boot-starter-tomcat-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.34/tomcat-embed-core-8.5.34.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/8.5.34/tomcat-embed-el-8.5.34.jar:/home/travis/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/8.5.34/tomcat-embed-websocket-8.5.34.jar:/home/travis/.m2/repository/org/hibernate/validator/hibernate-validator/6.0.12.Final/hibernate-validator-6.0.12.Final.jar:/home/travis/.m2/repository/javax/validation/validation-api/2.0.1.Final/validation-api-2.0.1.Final.jar:/home/travis/.m2/repository/org/jboss/logging/jboss-logging/3.3.2.Final/jboss-logging-3.3.2.Final.jar:/home/travis/.m2/repository/com/fasterxml/classmate/1.3.4/classmate-1.3.4.jar:/home/travis/.m2/repository/org/springframework/spring-web/5.0.9.RELEASE/spring-web-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-beans/5.0.9.RELEASE/spring-beans-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-webmvc/5.0.9.RELEASE/spring-webmvc-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-aop/5.0.9.RELEASE/spring-aop-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/springframework/spring-expression/5.0.9.RELEASE/spring-expression-5.0.9.RELEASE.jar:/home/travis/.m2/repository/com/google/guava/guava/23.3-jre/guava-23.3-jre.jar:/home/travis/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/travis/.m2/repository/com/google/errorprone/error_prone_annotations/2.0.18/error_prone_annotations-2.0.18.jar:/home/travis/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/travis/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/travis/.m2/repository/com/github/charithe/kafka-junit/4.1.1/kafka-junit-4.1.1.jar:/home/travis/.m2/repository/org/apache/curator/curator-test/4.0.0/curator-test-4.0.0.jar:/home/travis/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.1.1/junit-jupiter-api-5.1.1.jar:/home/travis/.m2/repository/org/apiguardian/apiguardian-api/1.0.0/apiguardian-api-1.0.0.jar:/home/travis/.m2/repository/org/opentest4j/opentest4j/1.0.0/opentest4j-1.0.0.jar:/home/travis/.m2/repository/org/junit/platform/junit-platform-commons/1.1.1/junit-platform-commons-1.1.1.jar:/home/travis/.m2/repository/com/squareup/okhttp3/mockwebserver/3.11.0/mockwebserver-3.11.0.jar:/home/travis/.m2/repository/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar:/home/travis/.m2/repository/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar:/home/travis/.m2/repository/org/apache/kafka/kafka_2.12/1.1.1/kafka_2.12-1.1.1.jar:/home/travis/.m2/repository/net/sf/jopt-simple/jopt-simple/4.9/jopt-simple-4.9.jar:/home/travis/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/travis/.m2/repository/org/scala-lang/scala-library/2.12.7/scala-library-2.12.7.jar:/home/travis/.m2/repository/org/scala-lang/scala-reflect/2.12.7/scala-reflect-2.12.7.jar:/home/travis/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.8.0/scala-logging_2.12-3.8.0.jar:/home/travis/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/travis/.m2/repository/org/apache/zookeeper/zookeeper/3.5.3-beta/zookeeper-3.5.3-beta.jar:/home/travis/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/travis/.m2/repository/org/apache/kafka/kafka_2.12/1.1.1/kafka_2.12-1.1.1-test.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-clients/1.1.1/kafka-clients-1.1.1-test.jar:/home/travis/.m2/repository/org/lz4/lz4-java/1.4.1/lz4-java-1.4.1.jar:/home/travis/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.1/snappy-java-1.1.7.1.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams/1.1.1/kafka-streams-1.1.1-test.jar:/home/travis/.m2/repository/org/apache/kafka/kafka-streams-test-utils/1.1.1/kafka-streams-test-utils-1.1.1.jar:/home/travis/.m2/repository/org/scalatest/scalatest_2.12/3.0.5/scalatest_2.12-3.0.5.jar:/home/travis/.m2/repository/org/scalactic/scalactic_2.12/3.0.5/scalactic_2.12-3.0.5.jar:/home/travis/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-starter-test/2.0.5.RELEASE/spring-boot-starter-test-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-test/2.0.5.RELEASE/spring-boot-test-2.0.5.RELEASE.jar:/home/travis/.m2/repository/org/springframework/boot/spring-boot-test-autoconfigure/2.0.5.RELEASE/spring-boot-test-autoconfigure-2.0.5.RELEASE.jar:/home/travis/.m2/repository/com/jayway/jsonpath/json-path/2.4.0/json-path-2.4.0.jar:/home/travis/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/home/travis/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/home/travis/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/home/travis/.m2/repository/org/assertj/assertj-core/3.9.1/assertj-core-3.9.1.jar:/home/travis/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/travis/.m2/repository/org/hamcrest/hamcrest-library/1.3/hamcrest-library-1.3.jar:/home/travis/.m2/repository/org/skyscreamer/jsonassert/1.5.0/jsonassert-1.5.0.jar:/home/travis/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar:/home/travis/.m2/repository/org/springframework/spring-test/5.0.9.RELEASE/spring-test-5.0.9.RELEASE.jar:/home/travis/.m2/repository/org/xmlunit/xmlunit-core/2.5.1/xmlunit-core-2.5.1.jar:/home/travis/.m2/repository/org/projectlombok/lombok/1.18.2/lombok-1.18.2.jar:/home/travis/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/travis/.m2/repository/io/micrometer/micrometer-registry-jmx/1.1.3/micrometer-registry-jmx-1.1.3.jar:/home/travis/.m2/repository/io/micrometer/micrometer-core/1.1.3/micrometer-core-1.1.3.jar:/home/travis/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.9/HdrHistogram-2.1.9.jar:/home/travis/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/home/travis/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.0.3/metrics-jmx-4.0.3.jar:/home/travis/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar:/home/travis/.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar:/home/travis/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/travis/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/travis/.m2/repository/org/mockito/mockito-core/2.23.0/mockito-core-2.23.0.jar:/home/travis/.m2/repository/net/bytebuddy/byte-buddy/1.7.11/byte-buddy-1.7.11.jar:/home/travis/.m2/repository/net/bytebuddy/byte-buddy-agent/1.7.11/byte-buddy-agent-1.7.11.jar:/home/travis/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.io.tmpdir=/tmp
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:java.compiler=<NA>
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.name=Linux
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.arch=amd64
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.version=4.4.0-101-generic
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:user.name=travis
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:user.home=/home/travis
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:user.dir=/home/travis/build/ExpediaDotCom/adaptive-alerting/kafka
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.memory.free=71MB
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.memory.max=1662MB
2019-03-18 23:25:51 INFO  ZooKeeper:109 - Client environment:os.memory.total=179MB
2019-03-18 23:25:51 INFO  ZooKeeper:865 - Initiating client connection, connectString=127.0.0.1:45726 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@46b0ccd9
2019-03-18 23:25:51 INFO  ClientCnxnSocket:236 - jute.maxbuffer value is 4194304 Bytes
2019-03-18 23:25:51 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Waiting until connected.
2019-03-18 23:25:51 INFO  ClientCnxn:1113 - Opening socket connection to server 127.0.0.1/127.0.0.1:45726. Will not attempt to authenticate using SASL (unknown error)
2019-03-18 23:25:51 INFO  ClientCnxn:948 - Socket connection established, initiating session, client: /127.0.0.1:58858, server: 127.0.0.1/127.0.0.1:45726
2019-03-18 23:25:51 INFO  NIOServerCnxnFactory:296 - Accepted socket connection from /127.0.0.1:58858
2019-03-18 23:25:51 INFO  ZooKeeperServer:1013 - Client attempting to establish new session at /127.0.0.1:58858
2019-03-18 23:25:51 INFO  FileTxnLog:204 - Creating new log file: log.1
2019-03-18 23:25:51 INFO  ZooKeeperServer:727 - Established session 0x1000002ed460000 with negotiated timeout 6000 for client /127.0.0.1:58858
2019-03-18 23:25:51 INFO  ClientCnxn:1381 - Session establishment complete on server 127.0.0.1/127.0.0.1:45726, sessionid = 0x1000002ed460000, negotiated timeout = 6000
2019-03-18 23:25:51 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Connected.
2019-03-18 23:25:52 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2019-03-18 23:25:52 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2019-03-18 23:25:52 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2019-03-18 23:25:52 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2019-03-18 23:25:52 INFO  KafkaServer:66 - Cluster ID = jypsdsL9SNa0wgmj8ZsCQQ
2019-03-18 23:25:52 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:43808
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:43808
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit7191447525342995375
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 43808
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:45726
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:25:52 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:43808
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:43808
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit7191447525342995375
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 43808
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:45726
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:25:53 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Starting
2019-03-18 23:25:53 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Starting
2019-03-18 23:25:53 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Starting
2019-03-18 23:25:53 INFO  LogManager:66 - Loading logs.
2019-03-18 23:25:53 INFO  LogManager:66 - Logs loading complete in 15 ms.
2019-03-18 23:25:53 INFO  LogManager:66 - Starting log cleanup with a period of 300000 ms.
2019-03-18 23:25:53 INFO  LogManager:66 - Starting log flusher with a default period of 9223372036854775807 ms.
2019-03-18 23:25:53 INFO  LogCleaner:66 - Starting the log cleaner
2019-03-18 23:25:53 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Starting
2019-03-18 23:25:53 INFO  Acceptor:66 - Awaiting socket connections on 0.0.0.0:43808.
2019-03-18 23:25:54 INFO  SocketServer:66 - [SocketServer brokerId=1] Started 1 acceptor threads
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Starting
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Starting
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Starting
2019-03-18 23:25:54 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Starting
2019-03-18 23:25:54 INFO  KafkaZkClient:66 - Creating /brokers/ids/1 (is it secure? false)
2019-03-18 23:25:54 INFO  KafkaZkClient:66 - Result of znode creation at /brokers/ids/1 is: OK
2019-03-18 23:25:54 INFO  KafkaZkClient:66 - Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(localhost,43808,ListenerName(PLAINTEXT),PLAINTEXT))
2019-03-18 23:25:54 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Starting
2019-03-18 23:25:54 INFO  KafkaZkClient:66 - Creating /controller (is it secure? false)
2019-03-18 23:25:54 INFO  KafkaZkClient:66 - Result of znode creation at /controller is: OK
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] 1 successfully elected as the controller
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Reading controller epoch from ZooKeeper
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Starting
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Starting
2019-03-18 23:25:54 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Starting
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Incrementing controller epoch in ZooKeeper
2019-03-18 23:25:54 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:setData cxid:0x20 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Epoch incremented to 1
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Registering handlers
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Deleting log dir event notifications
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Deleting isr change notifications
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Initializing controller context
2019-03-18 23:25:54 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Starting up.
2019-03-18 23:25:54 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Startup complete.
2019-03-18 23:25:54 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 35 milliseconds.
2019-03-18 23:25:54 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Partitions being reassigned: Map()
2019-03-18 23:25:54 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Starting up.
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Currently active brokers in the cluster: Set(1)
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Currently shutting brokers in the cluster: Set()
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Current list of topics in the cluster: Set()
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Fetching topic deletions in progress
2019-03-18 23:25:54 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Startup complete.
2019-03-18 23:25:54 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Starting
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] List of topics to be deleted: 
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] List of topics ineligible for deletion: 
2019-03-18 23:25:54 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Starting
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Initializing topic deletion manager
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Sending update metadata request
2019-03-18 23:25:54 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Controller 1 connected to localhost:43808 (id: 1 rack: null) for sending state change requests
2019-03-18 23:25:54 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Initializing replica state
2019-03-18 23:25:54 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Triggering online replica state changes
2019-03-18 23:25:54 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()
2019-03-18 23:25:54 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Initializing partition state
2019-03-18 23:25:54 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Triggering online partition state changes
2019-03-18 23:25:54 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Ready to serve as the new controller with epoch 1
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Partitions undergoing preferred replica election: 
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Partitions that completed preferred replica election: 
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Skipping preferred replica election for partitions due to topic deletion: 
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Resuming preferred replica election for partitions: 
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Starting preferred replica leader election for partitions 
2019-03-18 23:25:54 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:delete cxid:0x31 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2019-03-18 23:25:54 INFO  KafkaController:66 - [Controller id=1] Starting the controller scheduler
2019-03-18 23:25:54 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Starting
2019-03-18 23:25:54 INFO  SocketServer:66 - [SocketServer brokerId=1] Started processors for 1 acceptors
2019-03-18 23:25:54 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:25:54 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:25:54 INFO  KafkaServer:66 - [KafkaServer id=1] started
2019-03-18 23:25:55 INFO  Version:21 - HV000001: Hibernate Validator 6.0.12.Final

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.0.5.RELEASE)

2019-03-18 23:25:55 INFO  KafkaNotifierTest:50 - Starting KafkaNotifierTest on travis-job-94c84eda-d6a3-408d-97cc-f0c7c6a94056 with PID 5092 (started by travis in /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka)
2019-03-18 23:25:55 DEBUG KafkaNotifierTest:53 - Running with Spring Boot v2.0.5.RELEASE, Spring v5.0.9.RELEASE
2019-03-18 23:25:55 INFO  KafkaNotifierTest:680 - No active profile set, falling back to default profiles: default
2019-03-18 23:25:55 INFO  GenericWebApplicationContext:590 - Refreshing org.springframework.web.context.support.GenericWebApplicationContext@19993ed: startup date [Mon Mar 18 23:25:55 UTC 2019]; root of context hierarchy
2019-03-18 23:25:58 INFO  SimpleUrlHandlerMapping:373 - Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2019-03-18 23:25:58 INFO  RequestMappingHandlerAdapter:588 - Looking for @ControllerAdvice: org.springframework.web.context.support.GenericWebApplicationContext@19993ed: startup date [Mon Mar 18 23:25:55 UTC 2019]; root of context hierarchy
2019-03-18 23:25:59 INFO  RequestMappingHandlerMapping:549 - Mapped "{[/isActive],methods=[GET]}" onto public java.lang.Boolean com.expedia.adaptivealerting.kafka.notifier.HealthController.isActive()
2019-03-18 23:25:59 INFO  RequestMappingHandlerMapping:549 - Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
2019-03-18 23:25:59 INFO  RequestMappingHandlerMapping:549 - Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
2019-03-18 23:25:59 INFO  SimpleUrlHandlerMapping:373 - Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2019-03-18 23:25:59 INFO  SimpleUrlHandlerMapping:373 - Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2019-03-18 23:25:59 INFO  KafkaNotifierTest:59 - Started KafkaNotifierTest in 4.722 seconds (JVM running for 13.356)
2019-03-18 23:25:59 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:43808]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = aa_notifier
	heartbeat.interval.ms = 10000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:00 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:00 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:00 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:setData cxid:0x3e zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/alerts Error:KeeperErrorCode = NoNode for /config/topics/alerts
2019-03-18 23:26:00 INFO  AdminZkClient:66 - Topic creation Map(alerts-0 -> ArrayBuffer(1))
2019-03-18 23:26:00 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic alerts with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:00 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(alerts)], deleted topics: [Set()], new partition replica assignment [Map(alerts-0 -> Vector(1))]
2019-03-18 23:26:00 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x1000002ed460000 type:setData cxid:0x47 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2019-03-18 23:26:00 WARN  NetworkClient:882 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error while fetching metadata with correlation id 2 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:00 INFO  Metadata:265 - Cluster ID: jypsdsL9SNa0wgmj8ZsCQQ
2019-03-18 23:26:00 INFO  AdminZkClient:66 - Topic creation Map(__consumer_offsets-0 -> ArrayBuffer(1))
2019-03-18 23:26:00 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:00 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for alerts-0
2019-03-18 23:26:00 WARN  NetworkClient:882 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error while fetching metadata with correlation id 4 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:00 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]
2019-03-18 23:26:00 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for __consumer_offsets-0
2019-03-18 23:26:00 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions alerts-0
2019-03-18 23:26:00 WARN  NetworkClient:882 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error while fetching metadata with correlation id 6 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:00 WARN  NetworkClient:882 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error while fetching metadata with correlation id 8 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:00 INFO  Log:66 - [Log partition=alerts-0, dir=/tmp/kafka_junit7191447525342995375] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:00 INFO  Log:66 - [Log partition=alerts-0, dir=/tmp/kafka_junit7191447525342995375] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 116 ms
2019-03-18 23:26:00 INFO  LogManager:66 - Created log for partition alerts-0 in /tmp/kafka_junit7191447525342995375 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:00 INFO  Partition:66 - [Partition alerts-0 broker=1] No checkpointed highwatermark is found for partition alerts-0
2019-03-18 23:26:00 INFO  Replica:66 - Replica loaded for partition alerts-0 with initial high watermark 0
2019-03-18 23:26:00 WARN  NetworkClient:882 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error while fetching metadata with correlation id 10 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:00 INFO  Partition:66 - [Partition alerts-0 broker=1] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:00 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:00 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0
2019-03-18 23:26:00 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit7191447525342995375] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:00 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit7191447525342995375] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
2019-03-18 23:26:00 INFO  LogManager:66 - Created log for partition __consumer_offsets-0 in /tmp/kafka_junit7191447525342995375 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:00 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0
2019-03-18 23:26:00 INFO  Replica:66 - Replica loaded for partition __consumer_offsets-0 with initial high watermark 0
2019-03-18 23:26:00 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:00 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:00 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2019-03-18 23:26:00 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-1, groupId=aa_notifier] Discovered group coordinator localhost:43808 (id: 2147483646 rack: null)
2019-03-18 23:26:00 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-1, groupId=aa_notifier] Revoking previously assigned partitions []
2019-03-18 23:26:00 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-1, groupId=aa_notifier] (Re-)joining group
2019-03-18 23:26:00 INFO  AbstractCoordinator:657 - [Consumer clientId=consumer-1, groupId=aa_notifier] Group coordinator localhost:43808 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2019-03-18 23:26:00 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 100 milliseconds.
2019-03-18 23:26:00 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:43808]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:01 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:01 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:01 INFO  Metadata:265 - Cluster ID: jypsdsL9SNa0wgmj8ZsCQQ
2019-03-18 23:26:01 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-1, groupId=aa_notifier] Discovered group coordinator localhost:43808 (id: 2147483646 rack: null)
2019-03-18 23:26:01 INFO  AbstractCoordinator:657 - [Consumer clientId=consumer-1, groupId=aa_notifier] Group coordinator localhost:43808 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2019-03-18 23:26:01 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: alerts-0. Cache now contains 0 entries.
2019-03-18 23:26:01 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-1, groupId=aa_notifier] Discovered group coordinator localhost:43808 (id: 2147483646 rack: null)
2019-03-18 23:26:01 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-1, groupId=aa_notifier] (Re-)joining group
2019-03-18 23:26:01 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:01 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group aa_notifier with old generation 0 (__consumer_offsets-0)
2019-03-18 23:26:04 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group aa_notifier generation 1 (__consumer_offsets-0)
2019-03-18 23:26:04 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group aa_notifier for generation 1
2019-03-18 23:26:04 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.
2019-03-18 23:26:04 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-1, groupId=aa_notifier] Successfully joined group with generation 1
2019-03-18 23:26:04 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-1, groupId=aa_notifier] Setting newly assigned partitions [alerts-0]
2019-03-18 23:26:04 INFO  Fetcher:561 - [Consumer clientId=consumer-1, groupId=aa_notifier] Resetting offset for partition alerts-0 to offset 0.
Mar 18, 2019 11:26:04 PM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
INFO: MockWebServer[51515] received request: POST /hook HTTP/1.1 and responded: HTTP/1.1 200 OK
2019-03-18 23:26:05 INFO  EphemeralKafkaBroker:177 - Shutting down Kafka Server
2019-03-18 23:26:05 INFO  KafkaServer:66 - [KafkaServer id=1] shutting down
2019-03-18 23:26:05 INFO  KafkaServer:66 - [KafkaServer id=1] Starting controlled shutdown
2019-03-18 23:26:05 INFO  KafkaController:66 - [Controller id=1] Shutting down broker 1
2019-03-18 23:26:05 INFO  KafkaServer:66 - [KafkaServer id=1] Controlled shutdown succeeded
2019-03-18 23:26:05 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutting down
2019-03-18 23:26:05 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Stopped
2019-03-18 23:26:05 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutdown completed
2019-03-18 23:26:05 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopping socket server request processors
2019-03-18 23:26:05 INFO  AbstractCoordinator:657 - [Consumer clientId=consumer-1, groupId=aa_notifier] Group coordinator localhost:43808 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2019-03-18 23:26:05 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-1, groupId=aa_notifier] Error sending fetch request (sessionId=1482172453, epoch=1) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:05 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopped socket server request processors
2019-03-18 23:26:05 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shutting down
2019-03-18 23:26:05 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shut down completely
2019-03-18 23:26:05 INFO  KafkaApis:66 - [KafkaApi-1] Shutdown complete.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutting down
2019-03-18 23:26:05 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutdown completed
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Stopped
2019-03-18 23:26:05 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutting down.
2019-03-18 23:26:05 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0
2019-03-18 23:26:05 INFO  TransactionStateManager:66 - [Transaction State Manager 1]: Shutdown complete
2019-03-18 23:26:05 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutting down
2019-03-18 23:26:05 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Stopped
2019-03-18 23:26:05 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutdown completed
2019-03-18 23:26:05 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutdown complete.
2019-03-18 23:26:05 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutting down.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutting down
2019-03-18 23:26:05 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Stopped
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutdown completed
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutting down
2019-03-18 23:26:05 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Stopped
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutdown completed
2019-03-18 23:26:05 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutdown complete.
2019-03-18 23:26:05 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shutting down
2019-03-18 23:26:05 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutting down
2019-03-18 23:26:05 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Stopped
2019-03-18 23:26:05 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutdown completed
2019-03-18 23:26:05 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutting down
2019-03-18 23:26:05 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutdown completed
2019-03-18 23:26:05 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutting down
2019-03-18 23:26:05 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutdown completed
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutting down
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutdown completed
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutting down
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Stopped
2019-03-18 23:26:05 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Stopped
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutdown completed
2019-03-18 23:26:05 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutting down
2019-03-18 23:26:06 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutdown completed
2019-03-18 23:26:06 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Stopped
2019-03-18 23:26:06 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shut down completely
2019-03-18 23:26:06 INFO  LogManager:66 - Shutting down.
2019-03-18 23:26:06 INFO  LogCleaner:66 - Shutting down the log cleaner.
2019-03-18 23:26:06 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutting down
2019-03-18 23:26:06 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Stopped
2019-03-18 23:26:06 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutdown completed
2019-03-18 23:26:06 INFO  ProducerStateManager:66 - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 1
2019-03-18 23:26:06 INFO  ProducerStateManager:66 - [ProducerStateManager partition=alerts-0] Writing producer snapshot at offset 1
2019-03-18 23:26:06 INFO  LogManager:66 - Shutdown complete.
2019-03-18 23:26:06 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutting down
2019-03-18 23:26:06 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Stopped
2019-03-18 23:26:06 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutdown completed
2019-03-18 23:26:06 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Stopped partition state machine
2019-03-18 23:26:06 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Stopped replica state machine
2019-03-18 23:26:06 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutting down
2019-03-18 23:26:06 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Stopped
2019-03-18 23:26:06 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutdown completed
2019-03-18 23:26:06 INFO  KafkaController:66 - [Controller id=1] Resigned
2019-03-18 23:26:06 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closing.
2019-03-18 23:26:06 INFO  PrepRequestProcessor:613 - Processed session termination for sessionid: 0x1000002ed460000
2019-03-18 23:26:06 INFO  NIOServerCnxn:627 - Closed socket connection for client /127.0.0.1:58858 which had sessionid 0x1000002ed460000
2019-03-18 23:26:06 INFO  ClientCnxn:513 - EventThread shut down for session: 0x1000002ed460000
2019-03-18 23:26:06 INFO  ZooKeeper:1326 - Session: 0x1000002ed460000 closed
2019-03-18 23:26:06 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closed.
2019-03-18 23:26:06 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutting down
2019-03-18 23:26:06 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:07 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Stopped
2019-03-18 23:26:07 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutdown completed
2019-03-18 23:26:07 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutting down
2019-03-18 23:26:07 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutdown completed
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutting down
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Stopped
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Stopped
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutdown completed
2019-03-18 23:26:08 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutting down socket server
2019-03-18 23:26:08 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutdown completed
2019-03-18 23:26:08 INFO  KafkaServer:66 - [KafkaServer id=1] shut down completed
2019-03-18 23:26:08 INFO  EphemeralKafkaBroker:183 - Shutting down Zookeeper
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:583 - ConnnectionExpirerThread interrupted
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:219 - accept thread exitted run method
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:420 - selector thread exitted run method
2019-03-18 23:26:08 INFO  ZooKeeperServer:541 - shutting down
2019-03-18 23:26:08 INFO  SessionTrackerImpl:232 - Shutting down
2019-03-18 23:26:08 INFO  PrepRequestProcessor:1004 - Shutting down
2019-03-18 23:26:08 INFO  SyncRequestProcessor:191 - Shutting down
2019-03-18 23:26:08 INFO  SyncRequestProcessor:169 - SyncRequestProcessor exited!
2019-03-18 23:26:08 INFO  PrepRequestProcessor:156 - PrepRequestProcessor exited loop!
2019-03-18 23:26:08 INFO  FinalRequestProcessor:481 - shutdown of request processor complete
2019-03-18 23:26:08 INFO  EphemeralKafkaBroker:188 - Deleting the log dir:  /tmp/kafka_junit7191447525342995375
Mar 18, 2019 11:26:08 PM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
INFO: MockWebServer[51515] done accepting connections: Socket closed
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.77 s - in com.expedia.adaptivealerting.kafka.KafkaNotifierTest
[INFO] Running com.expedia.adaptivealerting.kafka.TypesafeConfigLoaderTest
2019-03-18 23:26:08 INFO  TypesafeConfigLoader:56 - Loading base configuration: appKey=ad-mapper
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.18 s - in com.expedia.adaptivealerting.kafka.TypesafeConfigLoaderTest
[INFO] Running com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest
2019-03-18 23:26:08 INFO  QuorumPeerConfig:327 - clientPortAddress is 0.0.0.0/0.0.0.0:42610
2019-03-18 23:26:08 INFO  QuorumPeerConfig:331 - secureClientPort is not set
2019-03-18 23:26:08 INFO  TestingZooKeeperMain:218 - Starting server
2019-03-18 23:26:08 INFO  ZooKeeperServer:907 - minSessionTimeout set to 6000
2019-03-18 23:26:08 INFO  ZooKeeperServer:916 - maxSessionTimeout set to 60000
2019-03-18 23:26:08 INFO  ZooKeeperServer:159 - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/1552951568443-0/version-2 snapdir /tmp/1552951568443-0/version-2
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:673 - Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 4 worker threads, and 64 kB direct buffers.
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:686 - binding to port 0.0.0.0/0.0.0.0:42610
2019-03-18 23:26:08 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951568443-0/version-2/snapshot.0
2019-03-18 23:26:08 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951568443-0/version-2/snapshot.0
2019-03-18 23:26:08 INFO  ContainerManager:64 - Using checkIntervalMs=60000 maxPerMinute=10000
2019-03-18 23:26:08 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:46348
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:46348
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit2458785817168641590
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 46348
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:42610
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:08 INFO  EphemeralKafkaBroker:144 - Starting Kafka server with config: {offsets.topic.replication.factor=1, advertised.listeners=PLAINTEXT://localhost:46348, default.replication.factor=1, transaction.state.log.replication.factor=1, offsets.topic.num.partitions=1, port=46348, transaction.state.log.min.isr=1, log.dirs=/tmp/kafka_junit2458785817168641590, group.min.session.timeout.ms=100, transaction.state.log.num.partitions=1, zookeeper.connect=127.0.0.1:42610, transaction.timeout.ms=500, leader.imbalance.check.interval.seconds=1, broker.id=1, num.partitions=1, listeners=PLAINTEXT://0.0.0.0:46348}
2019-03-18 23:26:08 INFO  KafkaServer:66 - starting
2019-03-18 23:26:08 INFO  KafkaServer:66 - Connecting to zookeeper on 127.0.0.1:42610
2019-03-18 23:26:08 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Initializing a new session to 127.0.0.1:42610.
2019-03-18 23:26:08 INFO  ZooKeeper:865 - Initiating client connection, connectString=127.0.0.1:42610 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@591409ad
2019-03-18 23:26:08 INFO  ClientCnxnSocket:236 - jute.maxbuffer value is 4194304 Bytes
2019-03-18 23:26:08 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Waiting until connected.
2019-03-18 23:26:08 INFO  ClientCnxn:1113 - Opening socket connection to server 127.0.0.1/127.0.0.1:42610. Will not attempt to authenticate using SASL (unknown error)
2019-03-18 23:26:08 INFO  NIOServerCnxnFactory:296 - Accepted socket connection from /127.0.0.1:48650
2019-03-18 23:26:08 INFO  ClientCnxn:948 - Socket connection established, initiating session, client: /127.0.0.1:48650, server: 127.0.0.1/127.0.0.1:42610
2019-03-18 23:26:08 INFO  ZooKeeperServer:1013 - Client attempting to establish new session at /127.0.0.1:48650
2019-03-18 23:26:08 INFO  FileTxnLog:204 - Creating new log file: log.1
2019-03-18 23:26:08 INFO  ZooKeeperServer:727 - Established session 0x100000336dd0000 with negotiated timeout 6000 for client /127.0.0.1:48650
2019-03-18 23:26:08 INFO  ClientCnxn:1381 - Session establishment complete on server 127.0.0.1/127.0.0.1:42610, sessionid = 0x100000336dd0000, negotiated timeout = 6000
2019-03-18 23:26:08 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Connected.
2019-03-18 23:26:08 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2019-03-18 23:26:08 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2019-03-18 23:26:08 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2019-03-18 23:26:08 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2019-03-18 23:26:08 INFO  KafkaServer:66 - Cluster ID = KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:08 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:46348
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:46348
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit2458785817168641590
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 46348
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:42610
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:08 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:46348
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:46348
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit2458785817168641590
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 46348
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:42610
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:08 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Starting
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Starting
2019-03-18 23:26:08 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Starting
2019-03-18 23:26:08 INFO  LogManager:66 - Loading logs.
2019-03-18 23:26:08 INFO  LogManager:66 - Logs loading complete in 1 ms.
2019-03-18 23:26:08 INFO  LogManager:66 - Starting log cleanup with a period of 300000 ms.
2019-03-18 23:26:08 INFO  LogManager:66 - Starting log flusher with a default period of 9223372036854775807 ms.
2019-03-18 23:26:08 INFO  LogCleaner:66 - Starting the log cleaner
2019-03-18 23:26:08 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Starting
2019-03-18 23:26:09 INFO  Acceptor:66 - Awaiting socket connections on 0.0.0.0:46348.
2019-03-18 23:26:09 INFO  SocketServer:66 - [SocketServer brokerId=1] Started 1 acceptor threads
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Starting
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Starting
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Starting
2019-03-18 23:26:09 INFO  KafkaZkClient:66 - Creating /brokers/ids/1 (is it secure? false)
2019-03-18 23:26:09 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Starting
2019-03-18 23:26:09 INFO  KafkaZkClient:66 - Result of znode creation at /brokers/ids/1 is: OK
2019-03-18 23:26:09 INFO  KafkaZkClient:66 - Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(localhost,46348,ListenerName(PLAINTEXT),PLAINTEXT))
2019-03-18 23:26:09 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Starting
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Starting
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Starting
2019-03-18 23:26:09 INFO  KafkaZkClient:66 - Creating /controller (is it secure? false)
2019-03-18 23:26:09 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Starting up.
2019-03-18 23:26:09 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Starting
2019-03-18 23:26:09 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Startup complete.
2019-03-18 23:26:09 INFO  KafkaZkClient:66 - Result of znode creation at /controller is: OK
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] 1 successfully elected as the controller
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Reading controller epoch from ZooKeeper
2019-03-18 23:26:09 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Incrementing controller epoch in ZooKeeper
2019-03-18 23:26:09 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:setData cxid:0x23 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2019-03-18 23:26:09 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Epoch incremented to 1
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Registering handlers
2019-03-18 23:26:09 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Starting up.
2019-03-18 23:26:09 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Startup complete.
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Deleting log dir event notifications
2019-03-18 23:26:09 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Starting
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Deleting isr change notifications
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Initializing controller context
2019-03-18 23:26:09 INFO  SocketServer:66 - [SocketServer brokerId=1] Started processors for 1 acceptors
2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  KafkaServer:66 - [KafkaServer id=1] started
2019-03-18 23:26:09 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Starting
2019-03-18 23:26:09 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Starting
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde$Ser

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Partitions being reassigned: Map()
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Currently active brokers in the cluster: Set(1)
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Currently shutting brokers in the cluster: Set()
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Current list of topics in the cluster: Set()
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Fetching topic deletions in progress
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] List of topics to be deleted: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] List of topics ineligible for deletion: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Initializing topic deletion manager
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Sending update metadata request
2019-03-18 23:26:09 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Controller 1 connected to localhost:46348 (id: 1 rack: null) for sending state change requests
2019-03-18 23:26:09 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:09 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Initializing replica state
2019-03-18 23:26:09 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Triggering online replica state changes
2019-03-18 23:26:09 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()
2019-03-18 23:26:09 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Initializing partition state
2019-03-18 23:26:09 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Triggering online partition state changes
2019-03-18 23:26:09 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Ready to serve as the new controller with epoch 1
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Partitions undergoing preferred replica election: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Partitions that completed preferred replica election: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Skipping preferred replica election for partitions due to topic deletion: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Resuming preferred replica election for partitions: 
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Starting preferred replica leader election for partitions 
2019-03-18 23:26:09 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:delete cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] Starting the controller scheduler
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde$Ser

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 WARN  AppInfoParser:66 - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=kafka-junit
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:450)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:303)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.buildMetricProducer(KafkaAnomalyToMetricMapperTest.java:162)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.setUp(KafkaAnomalyToMetricMapperTest.java:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 WARN  AppInfoParser:66 - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=kafka-junit
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:450)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318)
	at com.github.charithe.kafka.EphemeralKafkaBroker.createProducer(EphemeralKafkaBroker.java:301)
	at com.github.charithe.kafka.KafkaHelper.createProducer(KafkaHelper.java:129)
	at com.github.charithe.kafka.KafkaHelper.createStringProducer(KafkaHelper.java:138)
	at com.github.charithe.kafka.KafkaHelper.produceStrings(KafkaHelper.java:191)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.testRun(KafkaAnomalyToMetricMapperTest.java:117)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:09 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:setData cxid:0x3e zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/anomalies Error:KeeperErrorCode = NoNode for /config/topics/anomalies
2019-03-18 23:26:09 INFO  AdminZkClient:66 - Topic creation Map(anomalies-0 -> ArrayBuffer(1))
2019-03-18 23:26:09 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic anomalies with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(anomalies)], deleted topics: [Set()], new partition replica assignment [Map(anomalies-0 -> Vector(1))]
2019-03-18 23:26:09 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for anomalies-0
2019-03-18 23:26:09 WARN  NetworkClient:882 - [Producer clientId=kafka-junit] Error while fetching metadata with correlation id 1 : {anomalies=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions anomalies-0
2019-03-18 23:26:09 INFO  Log:66 - [Log partition=anomalies-0, dir=/tmp/kafka_junit2458785817168641590] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:09 INFO  Log:66 - [Log partition=anomalies-0, dir=/tmp/kafka_junit2458785817168641590] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms
2019-03-18 23:26:09 INFO  LogManager:66 - Created log for partition anomalies-0 in /tmp/kafka_junit2458785817168641590 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:09 INFO  Partition:66 - [Partition anomalies-0 broker=1] No checkpointed highwatermark is found for partition anomalies-0
2019-03-18 23:26:09 INFO  Replica:66 - Replica loaded for partition anomalies-0 with initial high watermark 0
2019-03-18 23:26:09 INFO  Partition:66 - [Partition anomalies-0 broker=1] anomalies-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:09 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:09 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:09 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: anomalies-0. Cache now contains 0 entries.
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:09 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:09 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:09 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:09 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:09 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:09 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapperTest:116 - Writing anomaly: {"metricData":{"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","interval":"1","org_id":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569},"detectorUuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","anomalyResult":{"anomalyLevel":"STRONG","predicted":null,"thresholds":null}}
2019-03-18 23:26:10 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:10 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:10 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:10 INFO  KafkaAnomalyToMetricMapper:107 - Starting KafkaAnomalyToMetricMapper
2019-03-18 23:26:10 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:10 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:setData cxid:0x4b zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2019-03-18 23:26:10 INFO  AdminZkClient:66 - Topic creation Map(__consumer_offsets-0 -> ArrayBuffer(1))
2019-03-18 23:26:10 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:10 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]
2019-03-18 23:26:10 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for __consumer_offsets-0
2019-03-18 23:26:10 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0
2019-03-18 23:26:10 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit2458785817168641590] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:10 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit2458785817168641590] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
2019-03-18 23:26:10 INFO  LogManager:66 - Created log for partition __consumer_offsets-0 in /tmp/kafka_junit2458785817168641590 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:10 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0
2019-03-18 23:26:10 INFO  Replica:66 - Replica loaded for partition __consumer_offsets-0 with initial high watermark 0
2019-03-18 23:26:10 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:10 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:10 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2019-03-18 23:26:10 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.
2019-03-18 23:26:10 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] Discovered group coordinator localhost:46348 (id: 2147483646 rack: null)
2019-03-18 23:26:10 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] Revoking previously assigned partitions []
2019-03-18 23:26:10 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] (Re-)joining group
2019-03-18 23:26:10 INFO  SessionTrackerImpl:158 - SessionTrackerImpl exited loop!
2019-03-18 23:26:10 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 0 (__consumer_offsets-0)
2019-03-18 23:26:11 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:12 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:13 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:13 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group kafka-junit-consumer generation 1 (__consumer_offsets-0)
2019-03-18 23:26:13 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group kafka-junit-consumer for generation 1
2019-03-18 23:26:13 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.
2019-03-18 23:26:13 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] Successfully joined group with generation 1
2019-03-18 23:26:13 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] Setting newly assigned partitions [anomalies-0]
2019-03-18 23:26:13 TRACE KafkaAnomalyToMetricMapper:131 - Read 0 anomaly records from topic=anomalies
2019-03-18 23:26:13 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 0 metricData records to topic=metrics
2019-03-18 23:26:13 INFO  Fetcher:561 - [Consumer clientId=consumer-3, groupId=kafka-junit-consumer] Resetting offset for partition anomalies-0 to offset 0.
2019-03-18 23:26:13 TRACE KafkaAnomalyToMetricMapper:131 - Read 20 anomaly records from topic=anomalies
2019-03-18 23:26:13 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:13 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x100000336dd0000 type:setData cxid:0x57 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics/metrics Error:KeeperErrorCode = NoNode for /config/topics/metrics
2019-03-18 23:26:13 INFO  AdminZkClient:66 - Topic creation Map(metrics-0 -> ArrayBuffer(1))
2019-03-18 23:26:13 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic metrics with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:13 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(metrics)], deleted topics: [Set()], new partition replica assignment [Map(metrics-0 -> Vector(1))]
2019-03-18 23:26:13 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for metrics-0
2019-03-18 23:26:13 WARN  NetworkClient:882 - [Producer clientId=kafka-junit] Error while fetching metadata with correlation id 1 : {metrics=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:13 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:13 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions metrics-0
2019-03-18 23:26:13 INFO  Log:66 - [Log partition=metrics-0, dir=/tmp/kafka_junit2458785817168641590] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:13 INFO  Log:66 - [Log partition=metrics-0, dir=/tmp/kafka_junit2458785817168641590] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms
2019-03-18 23:26:13 INFO  LogManager:66 - Created log for partition metrics-0 in /tmp/kafka_junit2458785817168641590 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:14 INFO  Partition:66 - [Partition metrics-0 broker=1] No checkpointed highwatermark is found for partition metrics-0
2019-03-18 23:26:14 INFO  Replica:66 - Replica loaded for partition metrics-0 with initial high watermark 0
2019-03-18 23:26:14 INFO  Partition:66 - [Partition metrics-0 broker=1] metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:14 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: metrics-0. Cache now contains 0 entries.
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=7097a470-3718-45eb-96ee-0beffe28dc6b, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951569}
2019-03-18 23:26:14 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 20 metricData records to topic=metrics
2019-03-18 23:26:14 TRACE KafkaAnomalyToMetricMapper:158 - timeDelay=5192
2019-03-18 23:26:14 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:15 TRACE KafkaAnomalyToMetricMapper:131 - Read 0 anomaly records from topic=anomalies
2019-03-18 23:26:15 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 0 metricData records to topic=metrics
2019-03-18 23:26:15 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:15 INFO  KafkaAnomalyToMetricMapper:116 - Stopping KafkaAnomalyToMetricMapper
2019-03-18 23:26:15 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2019-03-18 23:26:15 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 1 (__consumer_offsets-0)
2019-03-18 23:26:15 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Group kafka-junit-consumer with generation 2 is now empty (__consumer_offsets-0)
2019-03-18 23:26:15 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:15 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:15 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:15 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:15 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] Discovered group coordinator localhost:46348 (id: 2147483646 rack: null)
2019-03-18 23:26:15 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] Revoking previously assigned partitions []
2019-03-18 23:26:15 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] (Re-)joining group
2019-03-18 23:26:15 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 2 (__consumer_offsets-0)
2019-03-18 23:26:16 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:17 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:18 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:18 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group kafka-junit-consumer generation 3 (__consumer_offsets-0)
2019-03-18 23:26:18 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group kafka-junit-consumer for generation 3
2019-03-18 23:26:18 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] Successfully joined group with generation 3
2019-03-18 23:26:18 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] Setting newly assigned partitions [metrics-0]
2019-03-18 23:26:18 INFO  Fetcher:561 - [Consumer clientId=consumer-4, groupId=kafka-junit-consumer] Resetting offset for partition metrics-0 to offset 0.
2019-03-18 23:26:18 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 3 (__consumer_offsets-0)
2019-03-18 23:26:18 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Group kafka-junit-consumer with generation 4 is now empty (__consumer_offsets-0)
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapperTest:129 - metric={"metricDefinition":{"tags":{"kv":{"mtype":"gauge","unit":"","aa_anomaly_level":"STRONG","org_id":"1","aa_detector_uuid":"7097a470-3718-45eb-96ee-0beffe28dc6b","interval":"1"},"v":[]},"meta":{"kv":{},"v":[]},"key":"some-metric-key"},"value":100.0,"timestamp":1552951569}
2019-03-18 23:26:18 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:18 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:18 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:18 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde$Ser

2019-03-18 23:26:18 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:18 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:18 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Ser

2019-03-18 23:26:18 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:18 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:18 WARN  AppInfoParser:66 - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=kafka-junit
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:450)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:303)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.buildAnomalyProducer(KafkaAnomalyToMetricMapperTest.java:148)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.testRunSkipsUnmappedAnomalies(KafkaAnomalyToMetricMapperTest.java:194)
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.testRunSkipsUnmappedAnomalies(KafkaAnomalyToMetricMapperTest.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:18 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:18 INFO  KafkaAnomalyToMetricMapper:107 - Starting KafkaAnomalyToMetricMapper
2019-03-18 23:26:19 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:19 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-5, groupId=kafka-junit-consumer] Discovered group coordinator localhost:46348 (id: 2147483646 rack: null)
2019-03-18 23:26:19 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-5, groupId=kafka-junit-consumer] Revoking previously assigned partitions []
2019-03-18 23:26:19 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-5, groupId=kafka-junit-consumer] (Re-)joining group
2019-03-18 23:26:19 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 4 (__consumer_offsets-0)
2019-03-18 23:26:19 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:20 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:21 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:22 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group kafka-junit-consumer generation 5 (__consumer_offsets-0)
2019-03-18 23:26:22 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group kafka-junit-consumer for generation 5
2019-03-18 23:26:22 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-5, groupId=kafka-junit-consumer] Successfully joined group with generation 5
2019-03-18 23:26:22 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-5, groupId=kafka-junit-consumer] Setting newly assigned partitions [anomalies-0]
2019-03-18 23:26:22 TRACE KafkaAnomalyToMetricMapper:131 - Read 0 anomaly records from topic=anomalies
2019-03-18 23:26:22 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 0 metricData records to topic=metrics
2019-03-18 23:26:22 TRACE KafkaAnomalyToMetricMapper:131 - Read 15 anomaly records from topic=anomalies
2019-03-18 23:26:22 WARN  KafkaAnomalyToMetricMapper:193 - IllegalArgumentException: message=Metrictank does not support value [null] for key some-tag-key, newMetricDef=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=392bf81b-e93e-41e2-aa72-cad09722048c, some-tag-key=null, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}
2019-03-18 23:26:22 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=08a63b12-0401-4ceb-8317-b674d655352c, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951578}
2019-03-18 23:26:22 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:22 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=08a63b12-0401-4ceb-8317-b674d655352c, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951578}
2019-03-18 23:26:22 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=08a63b12-0401-4ceb-8317-b674d655352c, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951578}
2019-03-18 23:26:22 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=08a63b12-0401-4ceb-8317-b674d655352c, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951578}
2019-03-18 23:26:22 WARN  KafkaAnomalyToMetricMapper:193 - IllegalArgumentException: message=Metrictank does not support value [null] for key some-tag-key, newMetricDef=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=392bf81b-e93e-41e2-aa72-cad09722048c, some-tag-key=null, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}
2019-03-18 23:26:22 INFO  KafkaAnomalyToMetricMapper:182 - produced=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, aa_anomaly_level=STRONG, org_id=1, aa_detector_uuid=08a63b12-0401-4ceb-8317-b674d655352c, interval=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951578}
2019-03-18 23:26:22 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 5 metricData records to topic=metrics
2019-03-18 23:26:22 TRACE KafkaAnomalyToMetricMapper:158 - timeDelay=4133
2019-03-18 23:26:22 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:23 TRACE KafkaAnomalyToMetricMapper:131 - Read 0 anomaly records from topic=anomalies
2019-03-18 23:26:23 TRACE KafkaAnomalyToMetricMapper:148 - Wrote 0 metricData records to topic=metrics
2019-03-18 23:26:23 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:23 INFO  KafkaAnomalyToMetricMapper:116 - Stopping KafkaAnomalyToMetricMapper
2019-03-18 23:26:23 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde$Deser

2019-03-18 23:26:23 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:23 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:23 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 5 (__consumer_offsets-0)
2019-03-18 23:26:23 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Group kafka-junit-consumer with generation 6 is now empty (__consumer_offsets-0)
2019-03-18 23:26:23 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:24 INFO  Metadata:265 - Cluster ID: KlVGth9MSu2JQaoID_YwsA
2019-03-18 23:26:24 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-6, groupId=kafka-junit-consumer] Discovered group coordinator localhost:46348 (id: 2147483646 rack: null)
2019-03-18 23:26:24 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-6, groupId=kafka-junit-consumer] Revoking previously assigned partitions []
2019-03-18 23:26:24 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-6, groupId=kafka-junit-consumer] (Re-)joining group
2019-03-18 23:26:24 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 6 (__consumer_offsets-0)
2019-03-18 23:26:24 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:24 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:25 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:26 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group kafka-junit-consumer generation 7 (__consumer_offsets-0)
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group kafka-junit-consumer for generation 7
2019-03-18 23:26:27 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-6, groupId=kafka-junit-consumer] Successfully joined group with generation 7
2019-03-18 23:26:27 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-6, groupId=kafka-junit-consumer] Setting newly assigned partitions [metrics-0]
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group kafka-junit-consumer with old generation 7 (__consumer_offsets-0)
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Group kafka-junit-consumer with generation 8 is now empty (__consumer_offsets-0)
2019-03-18 23:26:27 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 10
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:46348]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 200
	fetch.min.bytes = 1
	group.id = kafka-junit-consumer
	heartbeat.interval.ms = 100
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 100
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 200
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:27 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:27 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:27 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:46348]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde$Ser

2019-03-18 23:26:27 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:27 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:27 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafkasvc:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:27 WARN  ClientUtils:56 - Removing server kafkasvc:9092 from bootstrap.servers as DNS resolution failed for kafkasvc
2019-03-18 23:26:27 INFO  EphemeralKafkaBroker:177 - Shutting down Kafka Server
2019-03-18 23:26:27 INFO  KafkaServer:66 - [KafkaServer id=1] shutting down
2019-03-18 23:26:27 INFO  KafkaServer:66 - [KafkaServer id=1] Starting controlled shutdown
2019-03-18 23:26:27 INFO  KafkaController:66 - [Controller id=1] Shutting down broker 1
2019-03-18 23:26:27 INFO  KafkaServer:66 - [KafkaServer id=1] Controlled shutdown succeeded
2019-03-18 23:26:27 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutting down
2019-03-18 23:26:27 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Stopped
2019-03-18 23:26:27 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutdown completed
2019-03-18 23:26:27 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopping socket server request processors
2019-03-18 23:26:27 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopped socket server request processors
2019-03-18 23:26:27 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shutting down
2019-03-18 23:26:27 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shut down completely
2019-03-18 23:26:27 INFO  KafkaApis:66 - [KafkaApi-1] Shutdown complete.
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutting down
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Stopped
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutdown completed
2019-03-18 23:26:27 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutting down.
2019-03-18 23:26:27 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0
2019-03-18 23:26:27 INFO  TransactionStateManager:66 - [Transaction State Manager 1]: Shutdown complete
2019-03-18 23:26:27 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutting down
2019-03-18 23:26:27 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Stopped
2019-03-18 23:26:27 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutdown completed
2019-03-18 23:26:27 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutdown complete.
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutting down.
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutting down
2019-03-18 23:26:27 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Stopped
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutdown completed
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutting down
2019-03-18 23:26:27 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutdown completed
2019-03-18 23:26:27 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutdown complete.
2019-03-18 23:26:27 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shutting down
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Stopped
2019-03-18 23:26:27 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutting down
2019-03-18 23:26:27 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Stopped
2019-03-18 23:26:27 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutdown completed
2019-03-18 23:26:27 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutting down
2019-03-18 23:26:27 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutdown completed
2019-03-18 23:26:27 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutting down
2019-03-18 23:26:27 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutdown completed
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutting down
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Stopped
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutdown completed
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutting down
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Stopped
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutdown completed
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutting down
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Stopped
2019-03-18 23:26:27 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutdown completed
2019-03-18 23:26:27 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shut down completely
2019-03-18 23:26:27 INFO  LogManager:66 - Shutting down.
2019-03-18 23:26:27 INFO  LogCleaner:66 - Shutting down the log cleaner.
2019-03-18 23:26:27 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutting down
2019-03-18 23:26:27 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Stopped
2019-03-18 23:26:27 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutdown completed
2019-03-18 23:26:27 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:27 INFO  ProducerStateManager:66 - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 339
2019-03-18 23:26:27 INFO  ProducerStateManager:66 - [ProducerStateManager partition=metrics-0] Writing producer snapshot at offset 25
2019-03-18 23:26:27 INFO  ProducerStateManager:66 - [ProducerStateManager partition=anomalies-0] Writing producer snapshot at offset 35
2019-03-18 23:26:27 INFO  LogManager:66 - Shutdown complete.
2019-03-18 23:26:27 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutting down
2019-03-18 23:26:27 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Stopped
2019-03-18 23:26:27 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutdown completed
2019-03-18 23:26:27 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Stopped partition state machine
2019-03-18 23:26:27 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Stopped replica state machine
2019-03-18 23:26:27 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutting down
2019-03-18 23:26:27 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Stopped
2019-03-18 23:26:27 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutdown completed
2019-03-18 23:26:27 INFO  KafkaController:66 - [Controller id=1] Resigned
2019-03-18 23:26:27 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closing.
2019-03-18 23:26:27 INFO  PrepRequestProcessor:613 - Processed session termination for sessionid: 0x100000336dd0000
2019-03-18 23:26:27 INFO  ZooKeeper:1326 - Session: 0x100000336dd0000 closed
2019-03-18 23:26:27 INFO  ClientCnxn:513 - EventThread shut down for session: 0x100000336dd0000
2019-03-18 23:26:27 INFO  NIOServerCnxn:627 - Closed socket connection for client /127.0.0.1:48650 which had sessionid 0x100000336dd0000
2019-03-18 23:26:27 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closed.
2019-03-18 23:26:27 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutting down
2019-03-18 23:26:28 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:28 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:28 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutdown completed
2019-03-18 23:26:28 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutting down
2019-03-18 23:26:28 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Stopped
2019-03-18 23:26:28 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:28 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:29 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Stopped
2019-03-18 23:26:29 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutdown completed
2019-03-18 23:26:29 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutting down
2019-03-18 23:26:29 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:29 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:30 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:30 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutdown completed
2019-03-18 23:26:30 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutting down socket server
2019-03-18 23:26:30 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Stopped
2019-03-18 23:26:30 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutdown completed
2019-03-18 23:26:30 INFO  KafkaServer:66 - [KafkaServer id=1] shut down completed
2019-03-18 23:26:30 INFO  EphemeralKafkaBroker:183 - Shutting down Zookeeper
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:583 - ConnnectionExpirerThread interrupted
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:420 - selector thread exitted run method
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:219 - accept thread exitted run method
2019-03-18 23:26:30 INFO  ZooKeeperServer:541 - shutting down
2019-03-18 23:26:30 INFO  SessionTrackerImpl:232 - Shutting down
2019-03-18 23:26:30 INFO  PrepRequestProcessor:1004 - Shutting down
2019-03-18 23:26:30 INFO  SyncRequestProcessor:191 - Shutting down
2019-03-18 23:26:30 INFO  PrepRequestProcessor:156 - PrepRequestProcessor exited loop!
2019-03-18 23:26:30 INFO  SyncRequestProcessor:169 - SyncRequestProcessor exited!
2019-03-18 23:26:30 INFO  FinalRequestProcessor:481 - shutdown of request processor complete
2019-03-18 23:26:30 INFO  EphemeralKafkaBroker:188 - Deleting the log dir:  /tmp/kafka_junit2458785817168641590
[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.337 s <<< FAILURE! - in com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest
[ERROR] testBuildMapper(com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest)  Time elapsed: 0.128 s  <<< ERROR!
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.testBuildMapper(KafkaAnomalyToMetricMapperTest.java:100)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyToMetricMapperTest.testBuildMapper(KafkaAnomalyToMetricMapperTest.java:100)

[INFO] Running com.expedia.adaptivealerting.kafka.notifier.NotifierTest
Mar 18, 2019 11:26:30 PM okhttp3.mockwebserver.MockWebServer$2 execute
INFO: MockWebServer[55592] starting to accept connections
2019-03-18 23:26:30 INFO  QuorumPeerConfig:327 - clientPortAddress is 0.0.0.0/0.0.0.0:43592
2019-03-18 23:26:30 INFO  QuorumPeerConfig:331 - secureClientPort is not set
2019-03-18 23:26:30 INFO  TestingZooKeeperMain:218 - Starting server
2019-03-18 23:26:30 INFO  ZooKeeperServer:907 - minSessionTimeout set to 6000
2019-03-18 23:26:30 INFO  ZooKeeperServer:916 - maxSessionTimeout set to 60000
2019-03-18 23:26:30 INFO  ZooKeeperServer:159 - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/1552951590800-0/version-2 snapdir /tmp/1552951590800-0/version-2
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:673 - Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 4 worker threads, and 64 kB direct buffers.
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:686 - binding to port 0.0.0.0/0.0.0.0:43592
2019-03-18 23:26:30 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951590800-0/version-2/snapshot.0
2019-03-18 23:26:30 INFO  FileTxnSnapLog:320 - Snapshotting: 0x0 to /tmp/1552951590800-0/version-2/snapshot.0
2019-03-18 23:26:30 INFO  ContainerManager:64 - Using checkIntervalMs=60000 maxPerMinute=10000
2019-03-18 23:26:30 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:45584
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:45584
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit9076392009312126167
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 45584
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:43592
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:30 INFO  EphemeralKafkaBroker:144 - Starting Kafka server with config: {offsets.topic.replication.factor=1, advertised.listeners=PLAINTEXT://localhost:45584, default.replication.factor=1, transaction.state.log.replication.factor=1, offsets.topic.num.partitions=1, port=45584, transaction.state.log.min.isr=1, log.dirs=/tmp/kafka_junit9076392009312126167, group.min.session.timeout.ms=100, transaction.state.log.num.partitions=1, zookeeper.connect=127.0.0.1:43592, transaction.timeout.ms=500, leader.imbalance.check.interval.seconds=1, broker.id=1, num.partitions=1, listeners=PLAINTEXT://0.0.0.0:45584}
2019-03-18 23:26:30 INFO  KafkaServer:66 - starting
2019-03-18 23:26:30 INFO  KafkaServer:66 - Connecting to zookeeper on 127.0.0.1:43592
2019-03-18 23:26:30 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Initializing a new session to 127.0.0.1:43592.
2019-03-18 23:26:30 INFO  ZooKeeper:865 - Initiating client connection, connectString=127.0.0.1:43592 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@42b1d43e
2019-03-18 23:26:30 INFO  ClientCnxnSocket:236 - jute.maxbuffer value is 4194304 Bytes
2019-03-18 23:26:30 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Waiting until connected.
2019-03-18 23:26:30 INFO  ClientCnxn:1113 - Opening socket connection to server 127.0.0.1/127.0.0.1:43592. Will not attempt to authenticate using SASL (unknown error)
2019-03-18 23:26:30 INFO  NIOServerCnxnFactory:296 - Accepted socket connection from /127.0.0.1:33918
2019-03-18 23:26:30 INFO  ClientCnxn:948 - Socket connection established, initiating session, client: /127.0.0.1:33918, server: 127.0.0.1/127.0.0.1:43592
2019-03-18 23:26:30 INFO  ZooKeeperServer:1013 - Client attempting to establish new session at /127.0.0.1:33918
2019-03-18 23:26:30 INFO  FileTxnLog:204 - Creating new log file: log.1
2019-03-18 23:26:30 INFO  ZooKeeperServer:727 - Established session 0x10000038e310000 with negotiated timeout 6000 for client /127.0.0.1:33918
2019-03-18 23:26:30 INFO  ClientCnxn:1381 - Session establishment complete on server 127.0.0.1/127.0.0.1:43592, sessionid = 0x10000038e310000, negotiated timeout = 6000
2019-03-18 23:26:30 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Connected.
2019-03-18 23:26:30 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
2019-03-18 23:26:30 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
2019-03-18 23:26:30 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
2019-03-18 23:26:30 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
2019-03-18 23:26:30 INFO  KafkaServer:66 - Cluster ID = Ypg-AmerTI2anBLbu0tuAg
2019-03-18 23:26:30 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:45584
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:45584
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit9076392009312126167
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 45584
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:43592
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:30 INFO  KafkaConfig:279 - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://localhost:45584
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 100
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 1
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:45584
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka_junit9076392009312126167
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 45584
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 1
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:43592
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

2019-03-18 23:26:30 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Starting
2019-03-18 23:26:30 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Starting
2019-03-18 23:26:30 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Starting
2019-03-18 23:26:30 INFO  LogManager:66 - Loading logs.
2019-03-18 23:26:30 INFO  LogManager:66 - Logs loading complete in 1 ms.
2019-03-18 23:26:30 INFO  LogManager:66 - Starting log cleanup with a period of 300000 ms.
2019-03-18 23:26:30 INFO  LogManager:66 - Starting log flusher with a default period of 9223372036854775807 ms.
2019-03-18 23:26:31 INFO  LogCleaner:66 - Starting the log cleaner
2019-03-18 23:26:31 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:31 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Starting
2019-03-18 23:26:31 INFO  Acceptor:66 - Awaiting socket connections on 0.0.0.0:45584.
2019-03-18 23:26:31 INFO  SocketServer:66 - [SocketServer brokerId=1] Started 1 acceptor threads
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Starting
2019-03-18 23:26:31 INFO  KafkaZkClient:66 - Creating /brokers/ids/1 (is it secure? false)
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Starting
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Starting
2019-03-18 23:26:31 INFO  KafkaZkClient:66 - Result of znode creation at /brokers/ids/1 is: OK
2019-03-18 23:26:31 INFO  KafkaZkClient:66 - Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(localhost,45584,ListenerName(PLAINTEXT),PLAINTEXT))
2019-03-18 23:26:31 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Starting
2019-03-18 23:26:31 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Starting
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Starting
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Starting
2019-03-18 23:26:31 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Starting
2019-03-18 23:26:31 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Starting up.
2019-03-18 23:26:31 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Startup complete.
2019-03-18 23:26:31 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds.
2019-03-18 23:26:31 INFO  KafkaZkClient:66 - Creating /controller (is it secure? false)
2019-03-18 23:26:31 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
2019-03-18 23:26:31 INFO  KafkaZkClient:66 - Result of znode creation at /controller is: OK
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] 1 successfully elected as the controller
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Reading controller epoch from ZooKeeper
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Incrementing controller epoch in ZooKeeper
2019-03-18 23:26:31 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:setData cxid:0x24 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
2019-03-18 23:26:31 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Starting up.
2019-03-18 23:26:31 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Startup complete.
2019-03-18 23:26:31 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Starting
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Epoch incremented to 1
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Registering handlers
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Deleting log dir event notifications
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Deleting isr change notifications
2019-03-18 23:26:31 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Starting
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Initializing controller context
2019-03-18 23:26:31 INFO  SocketServer:66 - [SocketServer brokerId=1] Started processors for 1 acceptors
2019-03-18 23:26:31 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:31 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:31 INFO  KafkaServer:66 - [KafkaServer id=1] started
2019-03-18 23:26:31 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Partitions being reassigned: Map()
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Currently active brokers in the cluster: Set(1)
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Currently shutting brokers in the cluster: Set()
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Current list of topics in the cluster: Set()
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Fetching topic deletions in progress
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] List of topics to be deleted: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] List of topics ineligible for deletion: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Initializing topic deletion manager
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Sending update metadata request
2019-03-18 23:26:31 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Initializing replica state
2019-03-18 23:26:31 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Triggering online replica state changes
2019-03-18 23:26:31 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map()
2019-03-18 23:26:31 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Initializing partition state
2019-03-18 23:26:31 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Triggering online partition state changes
2019-03-18 23:26:31 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map()
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Ready to serve as the new controller with epoch 1
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Partitions undergoing preferred replica election: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Partitions that completed preferred replica election: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Skipping preferred replica election for partitions due to topic deletion: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Resuming preferred replica election for partitions: 
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Starting preferred replica leader election for partitions 
2019-03-18 23:26:31 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:delete cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2019-03-18 23:26:31 INFO  KafkaController:66 - [Controller id=1] Starting the controller scheduler
2019-03-18 23:26:31 INFO  AnnotationConfigApplicationContext:590 - Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@2e191007: startup date [Mon Mar 18 23:26:31 UTC 2019]; root of context hierarchy
2019-03-18 23:26:31 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Starting
2019-03-18 23:26:31 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Controller 1 connected to localhost:45584 (id: 1 rack: null) for sending state change requests
2019-03-18 23:26:31 INFO  SessionTrackerImpl:158 - SessionTrackerImpl exited loop!
2019-03-18 23:26:32 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:32 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:32 INFO  AnnotationConfigApplicationContext:993 - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@2e191007: startup date [Mon Mar 18 23:26:31 UTC 2019]; root of context hierarchy
2019-03-18 23:26:32 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45584]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = aa_notifier
	heartbeat.interval.ms = 10000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:32 INFO  AnnotationConfigApplicationContext:590 - Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@6d6f2596: startup date [Mon Mar 18 23:26:32 UTC 2019]; root of context hierarchy
2019-03-18 23:26:32 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:32 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:32 INFO  AnnotationConfigApplicationContext:993 - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@6d6f2596: startup date [Mon Mar 18 23:26:32 UTC 2019]; root of context hierarchy
2019-03-18 23:26:32 INFO  AnnotationConfigApplicationContext:590 - Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@507c1e19: startup date [Mon Mar 18 23:26:32 UTC 2019]; root of context hierarchy
2019-03-18 23:26:32 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45584]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = aa_notifier
	heartbeat.interval.ms = 10000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:32 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:32 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:32 INFO  ConsumerConfig:279 - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:45584]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = aa_notifier
	heartbeat.interval.ms = 10000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde$Deser

2019-03-18 23:26:32 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:32 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:32 INFO  ProducerConfig:279 - ProducerConfig values: 
	acks = 1
	batch.size = 10
	bootstrap.servers = [localhost:45584]
	buffer.memory = 33554432
	client.id = kafka-junit
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 500
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-18 23:26:32 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:setData cxid:0x3e zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/alerts Error:KeeperErrorCode = NoNode for /config/topics/alerts
2019-03-18 23:26:32 INFO  AppInfoParser:109 - Kafka version : 1.1.1
2019-03-18 23:26:32 INFO  AppInfoParser:110 - Kafka commitId : 98b6346a977495f6
2019-03-18 23:26:32 WARN  AppInfoParser:66 - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=kafka-junit
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:62)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:450)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318)
	at com.github.charithe.kafka.EphemeralKafkaBroker.createProducer(EphemeralKafkaBroker.java:301)
	at com.github.charithe.kafka.KafkaHelper.createProducer(KafkaHelper.java:129)
	at com.github.charithe.kafka.KafkaHelper.createStringProducer(KafkaHelper.java:138)
	at com.github.charithe.kafka.KafkaHelper.produceStrings(KafkaHelper.java:191)
	at com.expedia.adaptivealerting.kafka.notifier.NotifierTest.message_invokesWebhook(NotifierTest.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:32 INFO  AdminZkClient:66 - Topic creation Map(alerts-0 -> ArrayBuffer(1))
2019-03-18 23:26:32 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic alerts with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:32 WARN  NetworkClient:882 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error while fetching metadata with correlation id 2 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:32 INFO  Metadata:265 - Cluster ID: Ypg-AmerTI2anBLbu0tuAg
2019-03-18 23:26:32 WARN  NetworkClient:882 - [Producer clientId=kafka-junit] Error while fetching metadata with correlation id 1 : {alerts=LEADER_NOT_AVAILABLE}
2019-03-18 23:26:32 INFO  PrepRequestProcessor:880 - Got user-level KeeperException when processing sessionid:0x10000038e310000 type:setData cxid:0x4a zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
2019-03-18 23:26:32 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(alerts)], deleted topics: [Set()], new partition replica assignment [Map(alerts-0 -> Vector(1))]
2019-03-18 23:26:32 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for alerts-0
2019-03-18 23:26:32 INFO  Metadata:265 - Cluster ID: Ypg-AmerTI2anBLbu0tuAg
2019-03-18 23:26:32 INFO  AdminZkClient:66 - Topic creation Map(__consumer_offsets-0 -> ArrayBuffer(1))
2019-03-18 23:26:32 INFO  KafkaApis:66 - [KafkaApi-1] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful
2019-03-18 23:26:33 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions alerts-0
2019-03-18 23:26:33 INFO  KafkaController:66 - [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> Vector(1))]
2019-03-18 23:26:33 INFO  KafkaController:66 - [Controller id=1] New partition creation callback for __consumer_offsets-0
2019-03-18 23:26:33 INFO  Log:66 - [Log partition=alerts-0, dir=/tmp/kafka_junit9076392009312126167] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:33 INFO  Log:66 - [Log partition=alerts-0, dir=/tmp/kafka_junit9076392009312126167] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms
2019-03-18 23:26:33 INFO  LogManager:66 - Created log for partition alerts-0 in /tmp/kafka_junit9076392009312126167 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:33 INFO  Partition:66 - [Partition alerts-0 broker=1] No checkpointed highwatermark is found for partition alerts-0
2019-03-18 23:26:33 INFO  Replica:66 - Replica loaded for partition alerts-0 with initial high watermark 0
2019-03-18 23:26:33 INFO  Partition:66 - [Partition alerts-0 broker=1] alerts-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:33 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:33 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions __consumer_offsets-0
2019-03-18 23:26:33 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit9076392009312126167] Loading producer state from offset 0 with message format version 2
2019-03-18 23:26:33 INFO  Log:66 - [Log partition=__consumer_offsets-0, dir=/tmp/kafka_junit9076392009312126167] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
2019-03-18 23:26:33 INFO  LogManager:66 - Created log for partition __consumer_offsets-0 in /tmp/kafka_junit9076392009312126167 with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}.
2019-03-18 23:26:33 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0
2019-03-18 23:26:33 INFO  Replica:66 - Replica loaded for partition __consumer_offsets-0 with initial high watermark 0
2019-03-18 23:26:33 INFO  Partition:66 - [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1
2019-03-18 23:26:33 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] Added fetcher for partitions List()
2019-03-18 23:26:33 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0
2019-03-18 23:26:33 INFO  GroupMetadataManager:66 - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds.
2019-03-18 23:26:33 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: alerts-0. Cache now contains 0 entries.
2019-03-18 23:26:33 INFO  KafkaProducer:1054 - [Producer clientId=kafka-junit] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2019-03-18 23:26:33 INFO  AbstractCoordinator:605 - [Consumer clientId=consumer-11, groupId=aa_notifier] Discovered group coordinator localhost:45584 (id: 2147483646 rack: null)
2019-03-18 23:26:33 INFO  ConsumerCoordinator:411 - [Consumer clientId=consumer-11, groupId=aa_notifier] Revoking previously assigned partitions []
2019-03-18 23:26:33 INFO  AbstractCoordinator:442 - [Consumer clientId=consumer-11, groupId=aa_notifier] (Re-)joining group
2019-03-18 23:26:33 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Preparing to rebalance group aa_notifier with old generation 0 (__consumer_offsets-0)
2019-03-18 23:26:33 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:33 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:34 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:34 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:35 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:35 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Stabilized group aa_notifier generation 1 (__consumer_offsets-0)
2019-03-18 23:26:36 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Assignment received from leader for group aa_notifier for generation 1
2019-03-18 23:26:36 INFO  LeaderEpochFileCache:66 - Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-0. Cache now contains 0 entries.
2019-03-18 23:26:36 INFO  AbstractCoordinator:409 - [Consumer clientId=consumer-11, groupId=aa_notifier] Successfully joined group with generation 1
2019-03-18 23:26:36 INFO  ConsumerCoordinator:256 - [Consumer clientId=consumer-11, groupId=aa_notifier] Setting newly assigned partitions [alerts-0]
2019-03-18 23:26:36 INFO  Fetcher:561 - [Consumer clientId=consumer-11, groupId=aa_notifier] Resetting offset for partition alerts-0 to offset 0.
2019-03-18 23:26:36 INFO  AnnotationConfigApplicationContext:993 - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@507c1e19: startup date [Mon Mar 18 23:26:32 UTC 2019]; root of context hierarchy
Mar 18, 2019 11:26:36 PM okhttp3.mockwebserver.MockWebServer$3 processOneRequest
INFO: MockWebServer[55592] received request: POST /hook HTTP/1.1 and responded: HTTP/1.1 200 OK
2019-03-18 23:26:36 INFO  EphemeralKafkaBroker:177 - Shutting down Kafka Server
2019-03-18 23:26:36 INFO  KafkaServer:66 - [KafkaServer id=1] shutting down
2019-03-18 23:26:36 INFO  KafkaServer:66 - [KafkaServer id=1] Starting controlled shutdown
2019-03-18 23:26:36 INFO  KafkaController:66 - [Controller id=1] Shutting down broker 1
2019-03-18 23:26:36 INFO  KafkaServer:66 - [KafkaServer id=1] Controlled shutdown succeeded
2019-03-18 23:26:36 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutting down
2019-03-18 23:26:36 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Stopped
2019-03-18 23:26:36 INFO  ZkNodeChangeNotificationListener$ChangeEventProcessThread:66 - [/config/changes-event-process-thread]: Shutdown completed
2019-03-18 23:26:36 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopping socket server request processors
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=1) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=INITIAL) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=INITIAL) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=INITIAL) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=INITIAL) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  FetchSessionHandler:440 - [Consumer clientId=consumer-11, groupId=aa_notifier] Error sending fetch request (sessionId=357459185, epoch=INITIAL) to node 1: org.apache.kafka.common.errors.DisconnectException.
2019-03-18 23:26:36 INFO  AbstractCoordinator:657 - [Consumer clientId=consumer-11, groupId=aa_notifier] Group coordinator localhost:45584 (id: 2147483646 rack: null) is unavailable or invalid, will attempt rediscovery
2019-03-18 23:26:36 INFO  SocketServer:66 - [SocketServer brokerId=1] Stopped socket server request processors
2019-03-18 23:26:36 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shutting down
2019-03-18 23:26:36 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 INFO  KafkaRequestHandlerPool:66 - [Kafka Request Handler on Broker 1], shut down completely
2019-03-18 23:26:36 INFO  KafkaApis:66 - [KafkaApi-1] Shutdown complete.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutting down
2019-03-18 23:26:36 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Shutdown completed
2019-03-18 23:26:36 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutting down.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-topic]: Stopped
2019-03-18 23:26:36 INFO  ProducerIdManager:66 - [ProducerId Manager 1]: Shutdown complete: last producerId assigned 0
2019-03-18 23:26:36 INFO  TransactionStateManager:66 - [Transaction State Manager 1]: Shutdown complete
2019-03-18 23:26:36 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutting down
2019-03-18 23:26:36 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Stopped
2019-03-18 23:26:36 INFO  TransactionMarkerChannelManager:66 - [Transaction Marker Channel Manager 1]: Shutdown completed
2019-03-18 23:26:36 INFO  TransactionCoordinator:66 - [TransactionCoordinator id=1] Shutdown complete.
2019-03-18 23:26:36 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutting down.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutting down
2019-03-18 23:26:36 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Shutdown completed
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Heartbeat]: Stopped
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutting down
2019-03-18 23:26:36 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Stopped
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Rebalance]: Shutdown completed
2019-03-18 23:26:36 INFO  GroupCoordinator:66 - [GroupCoordinator 1]: Shutdown complete.
2019-03-18 23:26:36 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shutting down
2019-03-18 23:26:36 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutting down
2019-03-18 23:26:36 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Stopped
2019-03-18 23:26:36 INFO  ReplicaManager$LogDirFailureHandler:66 - [LogDirFailureHandler]: Shutdown completed
2019-03-18 23:26:36 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutting down
2019-03-18 23:26:36 INFO  ReplicaFetcherManager:66 - [ReplicaFetcherManager on broker 1] shutdown completed
2019-03-18 23:26:36 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutting down
2019-03-18 23:26:36 INFO  ReplicaAlterLogDirsManager:66 - [ReplicaAlterLogDirsManager on broker 1] shutdown completed
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutting down
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Stopped
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Fetch]: Shutdown completed
2019-03-18 23:26:36 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutting down
2019-03-18 23:26:37 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:37 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Stopped
2019-03-18 23:26:37 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-Produce]: Shutdown completed
2019-03-18 23:26:37 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutting down
2019-03-18 23:26:37 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Stopped
2019-03-18 23:26:37 INFO  DelayedOperationPurgatory$ExpiredOperationReaper:66 - [ExpirationReaper-1-DeleteRecords]: Shutdown completed
2019-03-18 23:26:37 INFO  ReplicaManager:66 - [ReplicaManager broker=1] Shut down completely
2019-03-18 23:26:37 INFO  LogManager:66 - Shutting down.
2019-03-18 23:26:37 INFO  LogCleaner:66 - Shutting down the log cleaner.
2019-03-18 23:26:37 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutting down
2019-03-18 23:26:37 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Stopped
2019-03-18 23:26:37 INFO  LogCleaner:66 - [kafka-log-cleaner-thread-0]: Shutdown completed
2019-03-18 23:26:37 INFO  ProducerStateManager:66 - [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 1
2019-03-18 23:26:37 INFO  ProducerStateManager:66 - [ProducerStateManager partition=alerts-0] Writing producer snapshot at offset 1
2019-03-18 23:26:37 INFO  LogManager:66 - Shutdown complete.
2019-03-18 23:26:37 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutting down
2019-03-18 23:26:37 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Stopped
2019-03-18 23:26:37 INFO  ControllerEventManager$ControllerEventThread:66 - [ControllerEventThread controllerId=1] Shutdown completed
2019-03-18 23:26:37 INFO  PartitionStateMachine:66 - [PartitionStateMachine controllerId=1] Stopped partition state machine
2019-03-18 23:26:37 INFO  ReplicaStateMachine:66 - [ReplicaStateMachine controllerId=1] Stopped replica state machine
2019-03-18 23:26:37 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutting down
2019-03-18 23:26:37 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Stopped
2019-03-18 23:26:37 INFO  RequestSendThread:66 - [RequestSendThread controllerId=1] Shutdown completed
2019-03-18 23:26:37 INFO  KafkaController:66 - [Controller id=1] Resigned
2019-03-18 23:26:37 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closing.
2019-03-18 23:26:37 INFO  PrepRequestProcessor:613 - Processed session termination for sessionid: 0x10000038e310000
2019-03-18 23:26:37 INFO  ZooKeeper:1326 - Session: 0x10000038e310000 closed
2019-03-18 23:26:37 INFO  ClientCnxn:513 - EventThread shut down for session: 0x10000038e310000
2019-03-18 23:26:37 INFO  NIOServerCnxn:627 - Closed socket connection for client /127.0.0.1:33918 which had sessionid 0x10000038e310000
2019-03-18 23:26:37 INFO  ZooKeeperClient:66 - [ZooKeeperClient] Closed.
2019-03-18 23:26:37 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutting down
2019-03-18 23:26:37 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:37 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:37 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:38 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Stopped
2019-03-18 23:26:38 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Fetch]: Shutdown completed
2019-03-18 23:26:38 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutting down
2019-03-18 23:26:38 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:38 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:38 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:39 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Stopped
2019-03-18 23:26:39 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Produce]: Shutdown completed
2019-03-18 23:26:39 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutting down
2019-03-18 23:26:39 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:39 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:40 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:40 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Stopped
2019-03-18 23:26:40 INFO  ClientQuotaManager$ThrottledRequestReaper:66 - [ThrottledRequestReaper-Request]: Shutdown completed
2019-03-18 23:26:40 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutting down socket server
2019-03-18 23:26:40 INFO  SocketServer:66 - [SocketServer brokerId=1] Shutdown completed
2019-03-18 23:26:40 INFO  KafkaServer:66 - [KafkaServer id=1] shut down completed
2019-03-18 23:26:40 INFO  EphemeralKafkaBroker:183 - Shutting down Zookeeper
2019-03-18 23:26:40 INFO  NIOServerCnxnFactory:219 - accept thread exitted run method
2019-03-18 23:26:40 INFO  NIOServerCnxnFactory:583 - ConnnectionExpirerThread interrupted
2019-03-18 23:26:40 INFO  NIOServerCnxnFactory:420 - selector thread exitted run method
2019-03-18 23:26:40 INFO  ZooKeeperServer:541 - shutting down
2019-03-18 23:26:40 INFO  SessionTrackerImpl:232 - Shutting down
2019-03-18 23:26:40 INFO  PrepRequestProcessor:1004 - Shutting down
2019-03-18 23:26:40 INFO  PrepRequestProcessor:156 - PrepRequestProcessor exited loop!
2019-03-18 23:26:40 INFO  SyncRequestProcessor:191 - Shutting down
2019-03-18 23:26:40 INFO  SyncRequestProcessor:169 - SyncRequestProcessor exited!
2019-03-18 23:26:40 INFO  FinalRequestProcessor:481 - shutdown of request processor complete
2019-03-18 23:26:40 INFO  EphemeralKafkaBroker:188 - Deleting the log dir:  /tmp/kafka_junit9076392009312126167
Mar 18, 2019 11:26:40 PM okhttp3.mockwebserver.MockWebServer$2 acceptConnections
INFO: MockWebServer[55592] done accepting connections: Socket closed
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.419 s - in com.expedia.adaptivealerting.kafka.notifier.NotifierTest
[INFO] Running com.expedia.adaptivealerting.kafka.util.ConfigUtilTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 s - in com.expedia.adaptivealerting.kafka.util.ConfigUtilTest
[INFO] Running com.expedia.adaptivealerting.kafka.util.DetectorUtilTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.024 s - in com.expedia.adaptivealerting.kafka.util.DetectorUtilTest
[INFO] Running com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest
2019-03-18 23:26:40 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:40 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:40 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:40 ERROR AbstractJsonDeserializer:67 - Deserialization error
com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'invalid': was expecting ('true', 'false' or 'null')
 at [Source: (byte[])"invalid-input-value"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3532)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2627)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:832)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:729)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3091)
	at com.expedia.adaptivealerting.kafka.serde.AbstractJsonDeserializer.deserialize(AbstractJsonDeserializer.java:58)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55)
	at org.apache.kafka.streams.processor.internals.SourceNode.deserializeValue(SourceNode.java:56)
	at org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:61)
	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:91)
	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117)
	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:567)
	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:332)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.nullOnDeserException(KafkaDetectorManagerTest.java:202)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.nullOnDeserExceptionWithLogAndFailDriver(KafkaDetectorManagerTest.java:129)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:40 INFO  SessionTrackerImpl:158 - SessionTrackerImpl exited loop!
2019-03-18 23:26:40 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:40 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:40 INFO  KafkaAnomalyDetectorManager:94 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951600}, detectorUuid=17fd7214-ce3f-410b-a0a8-62e0793c3ced, anomalyResult=AnomalyResult(anomalyLevel=NORMAL, predicted=null, thresholds=null))
2019-03-18 23:26:40 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:40 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:40 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:94 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951600}, detectorUuid=7b17110e-1bc0-4947-8aad-7ececb3e4e76, anomalyResult=AnomalyResult(anomalyLevel=STRONG, predicted=null, thresholds=null))
2019-03-18 23:26:41 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 ERROR KafkaAnomalyDetectorManager:85 - Classification error: mappedMetricData=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951601}, detectorUuid=5248a10f-b37b-40a6-8894-f727c692bdf5, anomalyResult=null), error=java.lang.RuntimeException: Classification error
	at com.expedia.adaptivealerting.kafka.KafkaAnomalyDetectorManager.toAnomalyMmd(KafkaAnomalyDetectorManager.java:83)
	at org.apache.kafka.streams.kstream.internals.AbstractStream$2.apply(AbstractStream.java:127)
	at org.apache.kafka.streams.kstream.internals.KStreamMapValues$KStreamMapProcessor.process(KStreamMapValues.java:40)
	at org.apache.kafka.streams.processor.internals.ProcessorNode$1.run(ProcessorNode.java:46)
	at org.apache.kafka.streams.processor.internals.StreamsMetricsImpl.measureLatencyNs(StreamsMetricsImpl.java:211)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:124)
	at org.apache.kafka.streams.processor.internals.AbstractProcessorContext.forward(AbstractProcessorContext.java:174)
	at org.apache.kafka.streams.kstream.internals.KStreamFilter$KStreamFilterProcessor.process(KStreamFilter.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode$1.run(ProcessorNode.java:46)
	at org.apache.kafka.streams.processor.internals.StreamsMetricsImpl.measureLatencyNs(StreamsMetricsImpl.java:211)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:124)
	at org.apache.kafka.streams.processor.internals.AbstractProcessorContext.forward(AbstractProcessorContext.java:174)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:80)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:224)
	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:346)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.getAnomalyRecord(KafkaDetectorManagerTest.java:197)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.doesNotPublishAnomaly(KafkaDetectorManagerTest.java:192)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.testDoesNotPublishInvalidAnomaly(KafkaDetectorManagerTest.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:89 - anomalyResult=null
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:94 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951601}, detectorUuid=9577a6b1-fb4d-4668-bd48-deb5be24522c, anomalyResult=AnomalyResult(anomalyLevel=MODEL_WARMUP, predicted=null, thresholds=null))
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 ERROR AbstractJsonDeserializer:67 - Deserialization error
com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'invalid': was expecting ('true', 'false' or 'null')
 at [Source: (byte[])"invalid-input-value"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3532)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2627)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:832)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:729)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3091)
	at com.expedia.adaptivealerting.kafka.serde.AbstractJsonDeserializer.deserialize(AbstractJsonDeserializer.java:58)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55)
	at org.apache.kafka.streams.processor.internals.SourceNode.deserializeValue(SourceNode.java:56)
	at org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:61)
	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:91)
	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117)
	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:567)
	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:332)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.nullOnDeserException(KafkaDetectorManagerTest.java:202)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest.nullOnDeserExceptionWithLogAndContinueDriver(KafkaDetectorManagerTest.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:94 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951601}, detectorUuid=aed9d74a-e00f-4c5a-9c34-01141da49f90, anomalyResult=AnomalyResult(anomalyLevel=WEAK, predicted=null, thresholds=null))
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:66 - Initializing: inputTopic=mapped-metrics, outputTopic=anomalies
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:41 INFO  KafkaAnomalyDetectorManager:94 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951601}, detectorUuid=4a81bb5b-cd24-4abb-868c-8be61a8c56dd, anomalyResult=AnomalyResult(anomalyLevel=UNKNOWN, predicted=null, thresholds=null))
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.313 s - in com.expedia.adaptivealerting.kafka.KafkaDetectorManagerTest
[INFO] Running com.expedia.adaptivealerting.kafka.KafkaAnomalyToAlertMapperTest
2019-03-18 23:26:41 TRACE KafkaAnomalyToAlertMapperTest:103 - alert={
  "name" : "some-metric-key",
  "labels" : {
    "metric_key" : "some-metric-key",
    "mtype" : "gauge",
    "unit" : "",
    "org_id" : "1",
    "interval" : "1",
    "anomalyLevel" : "STRONG"
  },
  "annotations" : {
    "value" : "100.0",
    "timestamp" : "1552951601"
  },
  "creationTime" : 0,
  "generatorURL" : null
}
2019-03-18 23:26:41 WARN  NetworkClient:615 - [Producer clientId=kafka-junit] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:41 INFO  KafkaAnomalyToAlertMapper:66 - Initializing: inboundTopic=anomalies, outboundTopic=alert
2019-03-18 23:26:41 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:41 TRACE KafkaAnomalyToAlertMapperTest:87 - Output Record=ProducerRecord(topic=alert, partition=null, headers=RecordHeaders(headers = [], isReadOnly = false), key=e56e769f-a246-4e30-8636-6e0e5769d91e, value=Alert(name=some-metric-key, labels={metric_key=some-metric-key, mtype=gauge, unit=, interval=1, anomalyLevel=STRONG, org_id=1}, annotations={value=100.0, timestamp=1552951601}, creationTime=0, generatorURL=null), timestamp=1552951601729)
2019-03-18 23:26:41 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.188 s - in com.expedia.adaptivealerting.kafka.KafkaAnomalyToAlertMapperTest
[INFO] Running com.expedia.adaptivealerting.kafka.serde.MetricDataMessagePackSerdeTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 s - in com.expedia.adaptivealerting.kafka.serde.MetricDataMessagePackSerdeTest
[INFO] Running com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerdeTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.049 s - in com.expedia.adaptivealerting.kafka.serde.MappedMetricDataJsonSerdeTest
[INFO] Running com.expedia.adaptivealerting.kafka.serde.AlertJsonSerdeTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 s - in com.expedia.adaptivealerting.kafka.serde.AlertJsonSerdeTest
[INFO] Running com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerdeTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.033 s - in com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerdeTest
[INFO] Running com.expedia.adaptivealerting.kafka.processor.MappedMetricDataTimestampExtractorTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.expedia.adaptivealerting.kafka.processor.MappedMetricDataTimestampExtractorTest
[INFO] Running com.expedia.adaptivealerting.kafka.processor.MetricDataTimestampExtractorTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.expedia.adaptivealerting.kafka.processor.MetricDataTimestampExtractorTest
[INFO] Running com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest
2019-03-18 23:26:42 INFO  KafkaAnomalyDetectorMapper:76 - Initializing: inputTopic=metrics, outputTopic=mapped-metrics
2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 ERROR AbstractJsonDeserializer:67 - Deserialization error
com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'invalid': was expecting ('true', 'false' or 'null')
 at [Source: (byte[])"invalid-input-value"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3532)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2627)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:832)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:729)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3091)
	at com.expedia.adaptivealerting.kafka.serde.AbstractJsonDeserializer.deserialize(AbstractJsonDeserializer.java:58)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55)
	at org.apache.kafka.streams.processor.internals.SourceNode.deserializeValue(SourceNode.java:56)
	at org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:61)
	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:91)
	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117)
	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:567)
	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:332)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest.nullOnDeserException(KafkaDetectorMapperTest.java:156)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest.nullOnDeserExceptionWithLogAndFailDriver(KafkaDetectorMapperTest.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:42 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:42 INFO  KafkaAnomalyDetectorMapper:76 - Initializing: inputTopic=metrics, outputTopic=mapped-metrics
2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 WARN  NetworkClient:615 - [Consumer clientId=consumer-1, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 ERROR AbstractJsonDeserializer:67 - Deserialization error
com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'invalid': was expecting ('true', 'false' or 'null')
 at [Source: (byte[])"invalid-input-value"; line: 1, column: 9]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1804)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:703)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3532)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2627)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:832)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:729)
	at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4141)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4000)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3091)
	at com.expedia.adaptivealerting.kafka.serde.AbstractJsonDeserializer.deserialize(AbstractJsonDeserializer.java:58)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:65)
	at org.apache.kafka.common.serialization.ExtendedDeserializer$Wrapper.deserialize(ExtendedDeserializer.java:55)
	at org.apache.kafka.streams.processor.internals.SourceNode.deserializeValue(SourceNode.java:56)
	at org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:61)
	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:91)
	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117)
	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:567)
	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:332)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest.nullOnDeserException(KafkaDetectorMapperTest.java:156)
	at com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest.nullOnDeserExceptionWithLogAndContinueDriver(KafkaDetectorMapperTest.java:117)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2019-03-18 23:26:42 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
2019-03-18 23:26:42 INFO  KafkaAnomalyDetectorMapper:76 - Initializing: inputTopic=metrics, outputTopic=mapped-metrics
2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 INFO  StreamsConfig:279 - StreamsConfig values: 
	application.id = test
	application.server = 
	bootstrap.servers = [dummy:1234]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndContinueExceptionHandler
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class com.expedia.adaptivealerting.kafka.serde.MetricDataJsonSerde
	key.serde = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = null
	upgrade.from = null
	value.serde = null
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 

2019-03-18 23:26:42 INFO  KafkaAnomalyDetectorMapper:96 - produced=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951602}, detectorUuid=d84e0f47-d007-44f6-884e-3905e49d3032, anomalyResult=null)
2019-03-18 23:26:42 TRACE KafkaDetectorMapperTest:97 - outputRecord=ProducerRecord(topic=mapped-metrics, partition=null, headers=RecordHeaders(headers = [], isReadOnly = false), key=d84e0f47-d007-44f6-884e-3905e49d3032, value=MappedMetricData(metricData=MetricData{metricDefinition=MetricDefinition{key='some-metric-key', tags=TagCollection{kv={mtype=gauge, unit=, interval=1, org_id=1}, v=[]}, meta=TagCollection{kv={}, v=[]}}, value=100.0, timestamp=1552951602}, detectorUuid=d84e0f47-d007-44f6-884e-3905e49d3032, anomalyResult=null), timestamp=1552951602124)
2019-03-18 23:26:42 INFO  StateDirectory:281 - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.199 s - in com.expedia.adaptivealerting.kafka.KafkaDetectorMapperTest
2019-03-18 23:26:42 INFO  GenericWebApplicationContext:993 - Closing org.springframework.web.context.support.GenericWebApplicationContext@19993ed: startup date [Mon Mar 18 23:25:55 UTC 2019]; root of context hierarchy
2019-03-18 23:26:42 WARN  NetworkClient:615 - [Consumer clientId=consumer-11, groupId=aa_notifier] Connection to node 1 could not be established. Broker may not be available.
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   KafkaAnomalyToMetricMapperTest.testBuildMapper:100 » Kafka Failed to construct...
[INFO] 
[ERROR] Tests run: 39, Failures: 0, Errors: 1, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] com.expedia.adaptivealerting:adaptive-alerting 1.0.0-SNAPSHOT SUCCESS [  1.700 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-core  SUCCESS [ 11.510 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-anomdetect SUCCESS [ 25.813 s]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-kafka FAILURE [01:03 min]
[INFO] com.expedia.adaptivealerting:adaptive-alerting-modelservice SKIPPED
[INFO] com.expedia.adaptivealerting:adaptive-alerting-tools SKIPPED
[INFO] com.expedia.adaptivealerting:adaptive-alerting-samples SKIPPED
[INFO] com.expedia.adaptivealerting:adaptive-alerting-reporting 1.0.0-SNAPSHOT SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:44 min
[INFO] Finished at: 2019-03-18T23:26:42Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project adaptive-alerting-kafka: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/ExpediaDotCom/adaptive-alerting/kafka/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :adaptive-alerting-kafka
make: *** [build] Error 1
travis_time:end:148f69ff:start=1552951493337374876,finish=1552951602955156934,duration=109617782058
[0K[31;1mThe command "if ([ "$TRAVIS_BRANCH" == "master" ] && [ "$TRAVIS_PULL_REQUEST" == "false" ]) || [ -n "$TRAVIS_TAG" ]; then make release; else make all; fi" exited with 2.[0m

travis_fold:start:cache.2
[0Kstore build cache
travis_time:start:153b3350
[0Ktravis_time:end:153b3350:start=1552951602965097718,finish=1552951602968581739,duration=3484021
[0Ktravis_time:start:060b788f
[0K[32;1mchange detected (content changed, file is created, or file is deleted):
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting/1.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting/1.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT.jar
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-javadoc.jar
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/adaptive-alerting-anomdetect-1.0.0-SNAPSHOT-sources.jar
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/maven-metadata-local.xml
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptive-alerting-anomdetect/1.0.0-SNAPSHOT/_remote.repositories
/home/travis/.m2/repository/com/expedia/adaptivealerting/adaptiv
[0m
[32;1m...
[0m
[32;1mchanges detected, packing new archive[0m
.
.
.
.
.
.
[32;1muploading renamed-a2m-mapper/cache-linux-trusty-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855--jdk-oraclejdk8.tgz[0m
[32;1mcache uploaded[0m
travis_time:end:060b788f:start=1552951602973893427,finish=1552951647768577070,duration=44794683643
[0Ktravis_fold:end:cache.2
[0K

Done. Your build exited with 1.
